<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/amazonaws/services/sagemaker/sparksdk/algorithms/XGBoostSageMakerEstimator.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier;'>1 <span style=''>/*
</span>2 <span style=''> * Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.
</span>3 <span style=''> *
</span>4 <span style=''> * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;).
</span>5 <span style=''> * You may not use this file except in compliance with the License.
</span>6 <span style=''> * A copy of the License is located at
</span>7 <span style=''> *
</span>8 <span style=''> *   http://aws.amazon.com/apache2.0/
</span>9 <span style=''> *
</span>10 <span style=''> * or in the &quot;license&quot; file accompanying this file. This file is distributed
</span>11 <span style=''> * on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
</span>12 <span style=''> * express or implied. See the License for the specific language governing
</span>13 <span style=''> * permissions and limitations under the License.
</span>14 <span style=''> */
</span>15 <span style=''>
</span>16 <span style=''>package com.amazonaws.services.sagemaker.sparksdk.algorithms
</span>17 <span style=''>
</span>18 <span style=''>import com.amazonaws.regions.DefaultAwsRegionProviderChain
</span>19 <span style=''>import com.amazonaws.services.s3.{AmazonS3, AmazonS3ClientBuilder}
</span>20 <span style=''>import com.amazonaws.services.securitytoken.{AWSSecurityTokenService, AWSSecurityTokenServiceClientBuilder}
</span>21 <span style=''>
</span>22 <span style=''>import org.apache.spark.ml.param._
</span>23 <span style=''>import org.apache.spark.ml.util.Identifiable
</span>24 <span style=''>import org.apache.spark.sql.Row
</span>25 <span style=''>import org.apache.spark.sql.types.StructType
</span>26 <span style=''>
</span>27 <span style=''>import com.amazonaws.services.sagemaker.{AmazonSageMaker, AmazonSageMakerClientBuilder}
</span>28 <span style=''>import com.amazonaws.services.sagemaker.model.{S3DataDistribution, TrainingInputMode}
</span>29 <span style=''>import com.amazonaws.services.sagemaker.sparksdk._
</span>30 <span style=''>import com.amazonaws.services.sagemaker.sparksdk.EndpointCreationPolicy.EndpointCreationPolicy
</span>31 <span style=''>import com.amazonaws.services.sagemaker.sparksdk.transformation.{RequestRowSerializer, ResponseRowDeserializer}
</span>32 <span style=''>import com.amazonaws.services.sagemaker.sparksdk.transformation.deserializers.XGBoostCSVRowDeserializer
</span>33 <span style=''>import com.amazonaws.services.sagemaker.sparksdk.transformation.serializers.LibSVMRequestRowSerializer
</span>34 <span style=''>
</span>35 <span style=''>private[algorithms] trait XGBoostParams extends Params {
</span>36 <span style=''>
</span>37 <span style=''>  /* General parameters */
</span>38 <span style=''>  /** Which booster to use. Can be gbtree, gblinear or dart.
</span>39 <span style=''>    * The gbtree and dart values use a tree based model while gblinear uses a linear function.
</span>40 <span style=''>    * Default = gbtree
</span>41 <span style=''>    */
</span>42 <span style=''>  val booster : Param[String] = </span><span style='background: #AEF1AE'>new Param(this, &quot;booster&quot;,
</span>43 <span style=''></span><span style='background: #AEF1AE'>    &quot;Which booster to use. Can be gbtree, gblinear or dart. gbtree and dart use tree based model &quot; +
</span>44 <span style=''></span><span style='background: #AEF1AE'>    &quot;while gblinear uses linear function.&quot;,
</span>45 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;)))</span><span style=''>
</span>46 <span style=''>  def getBooster: String = </span><span style='background: #AEF1AE'>$(booster)</span><span style=''>
</span>47 <span style=''>
</span>48 <span style=''>  /** Whether in silent mode. Can be 0 or 1.
</span>49 <span style=''>    * 0 means printing running messages, 1 means silent mode.
</span>50 <span style=''>    * Default = 0
</span>51 <span style=''>    */
</span>52 <span style=''>  val silent : IntParam = </span><span style='background: #AEF1AE'>new IntParam(this, &quot;silent&quot;,
</span>53 <span style=''></span><span style='background: #AEF1AE'>    &quot;Whether in silent mode. Can be 0 or 1. &quot; +
</span>54 <span style=''></span><span style='background: #AEF1AE'>    &quot;0 means print running messages, 1 means silent mode.&quot;,
</span>55 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(0, 1)))</span><span style=''>
</span>56 <span style=''>  def getSilent: Int = </span><span style='background: #AEF1AE'>$(silent)</span><span style=''>
</span>57 <span style=''>
</span>58 <span style=''>  /** Number of parallel threads used to run xgboost. Must be &gt;= 1.
</span>59 <span style=''>    * Defaults to maximum number of threads available.
</span>60 <span style=''>    */
</span>61 <span style=''>  val nThread: IntParam = </span><span style='background: #AEF1AE'>new IntParam(this, &quot;nthread&quot;,
</span>62 <span style=''></span><span style='background: #AEF1AE'>    &quot;Number of parallel threads used to run xgboost. Must be &gt;= 1. &quot;,
</span>63 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.gtEq(1))</span><span style=''>
</span>64 <span style=''>  def getNThread: Int = </span><span style='background: #AEF1AE'>$(nThread)</span><span style=''>
</span>65 <span style=''>
</span>66 <span style=''>  /* Booster Parameters */
</span>67 <span style=''>  /** Step size shrinkage used in update to prevent overfitting. After each boosting step, we can
</span>68 <span style=''>    * directly get the weights of new features and eta actually shrinks the feature weights to make
</span>69 <span style=''>    * the boosting process more conservative. Must be in [0, 1]
</span>70 <span style=''>    * Default = 0.3
</span>71 <span style=''>    */
</span>72 <span style=''>  val eta: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;eta&quot;,
</span>73 <span style=''></span><span style='background: #AEF1AE'>    &quot;Step size shrinkage used in update to prevent overfitting. After each boosting step, &quot; +
</span>74 <span style=''></span><span style='background: #AEF1AE'>    &quot;we can directly get the weights of new features. and eta shrinks the feature &quot; +
</span>75 <span style=''></span><span style='background: #AEF1AE'>    &quot;weights to make the boosting process more conservative. Must be in [0, 1]. &quot;,
</span>76 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inRange(0, 1))</span><span style=''>
</span>77 <span style=''>  def getEta: Double = </span><span style='background: #AEF1AE'>$(eta)</span><span style=''>
</span>78 <span style=''>
</span>79 <span style=''>  /** Minimum loss reduction required to make a further partition on a leaf node of the tree.
</span>80 <span style=''>    * The larger, the more conservative the algorithm will be. Must be &gt;= 0.
</span>81 <span style=''>    * Default = 0
</span>82 <span style=''>    */
</span>83 <span style=''>  val gamma: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;gamma&quot;,
</span>84 <span style=''></span><span style='background: #AEF1AE'>    &quot;Minimum loss reduction required to make an additional partition on a leaf node of the tree. &quot; +
</span>85 <span style=''></span><span style='background: #AEF1AE'>    &quot;The larger the value, the more conservative the algorithm will be. Must be &gt;= 0.&quot;,
</span>86 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.gtEq(0))</span><span style=''>
</span>87 <span style=''>  def getGamma: Double = </span><span style='background: #AEF1AE'>$(gamma)</span><span style=''>
</span>88 <span style=''>
</span>89 <span style=''>  /** Maximum depth of a tree, increase this value will make the model more complex (likely to be
</span>90 <span style=''>    * overfitting). 0 indicates no limit, limit is required when grow_policy=depth-wise.
</span>91 <span style=''>    * Must be &gt;= 0.
</span>92 <span style=''>    * Default = 6
</span>93 <span style=''>    */
</span>94 <span style=''>  val maxDepth: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;max_depth&quot;,
</span>95 <span style=''></span><span style='background: #AEF1AE'>    &quot; Maximum depth of a tree, increase this value will make the model more complex (likely to be&quot; +
</span>96 <span style=''></span><span style='background: #AEF1AE'>      &quot; overfitting). 0 indicates no limit, limit is required when grow_policy=depth-wise. &quot; +
</span>97 <span style=''></span><span style='background: #AEF1AE'>      &quot;Must be &gt;= 0. &quot;,
</span>98 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.gtEq(0))</span><span style=''>
</span>99 <span style=''>  def getMaxDepth: Double = </span><span style='background: #AEF1AE'>$(maxDepth)</span><span style=''>
</span>100 <span style=''>
</span>101 <span style=''>  /** Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results
</span>102 <span style=''>    * in a leaf node with the sum of instance weight less than min_child_weight, then the building
</span>103 <span style=''>    * process will give up further partitioning. In linear regression mode, this simply corresponds
</span>104 <span style=''>    * to minimum number of instances needed to be in each node. The larger the algorithm is,
</span>105 <span style=''>    * the more conservative it will be. Must be &gt;= 0.
</span>106 <span style=''>    * Default = 1
</span>107 <span style=''>    */
</span>108 <span style=''>  val minChildWeight: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;min_child_weight&quot;,
</span>109 <span style=''></span><span style='background: #AEF1AE'>    &quot;Minimum sum of instance weight (hessian) needed in a child. If the tree partition step &quot; +
</span>110 <span style=''></span><span style='background: #AEF1AE'>    &quot;results in a leaf node with the sum of instance weight less than min_child_weight, then &quot; +
</span>111 <span style=''></span><span style='background: #AEF1AE'>    &quot;the building process will give up further partitioning. In linear regression mode, this &quot; +
</span>112 <span style=''></span><span style='background: #AEF1AE'>    &quot;simply corresponds to minimum number of instances needed to be in each node. The larger the &quot; +
</span>113 <span style=''></span><span style='background: #AEF1AE'>    &quot;value, the more conservative the algorithm will be. Must be &gt;= 0.&quot;, ParamValidators.gtEq(0))</span><span style=''>
</span>114 <span style=''>  def getMinChildWeight: Double = </span><span style='background: #AEF1AE'>$(minChildWeight)</span><span style=''>
</span>115 <span style=''>
</span>116 <span style=''>  /** Maximum delta step allowed for each tree's weight estimation can be. Valid inputs: When a
</span>117 <span style=''>    * positive integer is used, it helps make the update more conservative. The preferred options
</span>118 <span style=''>    * is to use it in logistic regression. Set it to 1-10 to help control the update. Must be &gt;= 0.
</span>119 <span style=''>    * Default = 0
</span>120 <span style=''>    */
</span>121 <span style=''>  val maxDeltaStep: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;max_delta_step&quot;,
</span>122 <span style=''></span><span style='background: #AEF1AE'>    &quot;Maximum delta step allowed for each tree's weight estimation to be. If the value is set to&quot; +
</span>123 <span style=''></span><span style='background: #AEF1AE'>    &quot; 0, it means there is no constraint. If it is set to a positive value, it can help make the &quot; +
</span>124 <span style=''></span><span style='background: #AEF1AE'>    &quot;update step more conservative. Usually this parameter is not needed, but it might help &quot; +
</span>125 <span style=''></span><span style='background: #AEF1AE'>    &quot;in logistic regression when the classes are extremely imbalanced. Setting it to value of &quot; +
</span>126 <span style=''></span><span style='background: #AEF1AE'>    &quot;1-10 might help control the update. Must be &gt;= 0.&quot;, ParamValidators.gtEq(0))</span><span style=''>
</span>127 <span style=''>  def getMaxDeltaStep: Double = </span><span style='background: #AEF1AE'>$(maxDeltaStep)</span><span style=''>
</span>128 <span style=''>
</span>129 <span style=''>  /** Subsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly
</span>130 <span style=''>    * collected half of the data instances to grow trees and this will prevent overfitting.
</span>131 <span style=''>    * Must be in (0, 1].
</span>132 <span style=''>    * Default = 1
</span>133 <span style=''>    */
</span>134 <span style=''>  val subsample: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;subsample&quot;,
</span>135 <span style=''></span><span style='background: #AEF1AE'>    &quot;Subsample ratio of the training instance. Setting it to 0.5 means that XGBoost will &quot; +
</span>136 <span style=''></span><span style='background: #AEF1AE'>    &quot;randomly collect half of the data instances to grow trees and this will prevent overfitting.&quot; +
</span>137 <span style=''></span><span style='background: #AEF1AE'>      &quot;Must be in (0, 1]. &quot;,
</span>138 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inRange(0, 1, lowerInclusive = false, upperInclusive = true))</span><span style=''>
</span>139 <span style=''>  def getSubsample: Double = </span><span style='background: #AEF1AE'>$(subsample)</span><span style=''>
</span>140 <span style=''>
</span>141 <span style=''>
</span>142 <span style=''>  /** Subsample ratio of columns when constructing each tree. Must be in (0, 1]
</span>143 <span style=''>    * Default = 1
</span>144 <span style=''>    */
</span>145 <span style=''>  val colSampleByTree: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;colsample_bytree&quot;,
</span>146 <span style=''></span><span style='background: #AEF1AE'>    &quot;Subsample ratio of columns when constructing each tree. Must be in (0, 1]&quot;,
</span>147 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inRange(0, 1, lowerInclusive = false, upperInclusive = true))</span><span style=''>
</span>148 <span style=''>  def getColSampleByTree: Double = </span><span style='background: #AEF1AE'>$(colSampleByTree)</span><span style=''>
</span>149 <span style=''>
</span>150 <span style=''>  /** Subsample ratio of columns for each split, in each level. Must be in (0, 1].
</span>151 <span style=''>    * Default = 1
</span>152 <span style=''>    */
</span>153 <span style=''>  val colSampleByLevel: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;colsample_bylevel&quot;,
</span>154 <span style=''></span><span style='background: #AEF1AE'>    &quot;Subsample ratio of columns for each split, in each level. Must be in (0, 1].&quot;,
</span>155 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inRange(0, 1, lowerInclusive = false, upperInclusive = true))</span><span style=''>
</span>156 <span style=''>  def getColSampleByLevel: Double = </span><span style='background: #AEF1AE'>$(colSampleByLevel)</span><span style=''>
</span>157 <span style=''>
</span>158 <span style=''>  /** L2 regularization term on weights. Increase this value will make model more conservative.
</span>159 <span style=''>    * Default = 1
</span>160 <span style=''>    */
</span>161 <span style=''>  val lambda: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;lambda&quot;,
</span>162 <span style=''></span><span style='background: #AEF1AE'>    &quot;L2 regularization term on weights, increase this value will make model more conservative.&quot;)</span><span style=''>
</span>163 <span style=''>  def getLambda: Double = </span><span style='background: #AEF1AE'>$(lambda)</span><span style=''>
</span>164 <span style=''>
</span>165 <span style=''>  /** L1 regularization term on weights. Increase this value will make model more conservative.
</span>166 <span style=''>    * Default = 0
</span>167 <span style=''>    */
</span>168 <span style=''>  val alpha: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;alpha&quot;,
</span>169 <span style=''></span><span style='background: #AEF1AE'>    &quot;L1 regularization term on weights, increase this value will make model more conservative.&quot;)</span><span style=''>
</span>170 <span style=''>  def getAlpha: Double = </span><span style='background: #AEF1AE'>$(alpha)</span><span style=''>
</span>171 <span style=''>
</span>172 <span style=''>  /** The tree construction algorithm used in XGBoost. Can be auto, exact, approx, hist.
</span>173 <span style=''>    * Default = &quot;auto&quot;
</span>174 <span style=''>    */
</span>175 <span style=''>  val treeMethod: Param[String] = </span><span style='background: #AEF1AE'>new Param(this, &quot;tree_method&quot;,
</span>176 <span style=''></span><span style='background: #AEF1AE'>    &quot;The tree construction algorithm used in XGBoost. Can be auto, exact, approx, hist.&quot;,
</span>177 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(&quot;auto&quot;, &quot;exact&quot;, &quot;approx&quot;, &quot;hist&quot;)))</span><span style=''>
</span>178 <span style=''>  def getTreeMethod: String = </span><span style='background: #AEF1AE'>$(treeMethod)</span><span style=''>
</span>179 <span style=''>
</span>180 <span style=''>  /** Used only for approximate greedy algorithm. Translates into O(1 / sketch_eps) number of
</span>181 <span style=''>    * bins. Compared to directly select number of bins, this comes with theoretical guarantee with
</span>182 <span style=''>    * sketch accuracy. Must be in (0, 1).
</span>183 <span style=''>    * Default = 0.03
</span>184 <span style=''>    */
</span>185 <span style=''>  val sketchEps: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;sketch_eps&quot;,
</span>186 <span style=''></span><span style='background: #AEF1AE'>    &quot;Used only for approximate greedy algorithm. Translates into &quot; +
</span>187 <span style=''></span><span style='background: #AEF1AE'>    &quot;O(1 / sketch_eps) number of bins. Compared to directly select number of bins, &quot; +
</span>188 <span style=''></span><span style='background: #AEF1AE'>    &quot;this comes with theoretical guarantee with sketch accuracy. Must be in (0, 1). &quot;,
</span>189 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inRange(0, 1, lowerInclusive = false, upperInclusive = false))</span><span style=''>
</span>190 <span style=''>  def getSketchEps: Double = </span><span style='background: #AEF1AE'>$(sketchEps)</span><span style=''>
</span>191 <span style=''>
</span>192 <span style=''>  /** Controls the balance of positive and negative weights. It's useful for unbalanced classes.
</span>193 <span style=''>    * A typical value to consider: sum(negative cases) / sum(positive cases).
</span>194 <span style=''>    * Default = 1
</span>195 <span style=''>    */
</span>196 <span style=''>  val scalePosWeight: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;scale_pos_weight&quot;,
</span>197 <span style=''></span><span style='background: #AEF1AE'>    &quot;Scale the weight of positive examples by this factor. Useful for unbalanced classes&quot;)</span><span style=''>
</span>198 <span style=''>  def getScalePosWeight: Double = </span><span style='background: #AEF1AE'>$(scalePosWeight)</span><span style=''>
</span>199 <span style=''>
</span>200 <span style=''>  /** A comma-separated string that defines the sequence of tree updaters to run.
</span>201 <span style=''>    * This provides a modular way to construct and to modify the trees.
</span>202 <span style=''>    * Default = &quot;grow_colmaker,prune&quot;
</span>203 <span style=''>    */
</span>204 <span style=''>  val updater: Param[String] = </span><span style='background: #AEF1AE'>new Param(this, &quot;updater&quot;,
</span>205 <span style=''></span><span style='background: #AEF1AE'>    &quot;A comma separated string defining the sequence of tree updaters to run, providing a modular &quot; +
</span>206 <span style=''></span><span style='background: #AEF1AE'>    &quot;way to construct and to modify the trees. This is an advanced parameter that &quot; +
</span>207 <span style=''></span><span style='background: #AEF1AE'>    &quot;is usually set automatically, depending on some other parameters&quot;, updaterValidator)</span><span style=''>
</span>208 <span style=''>  def getUpdater: String = </span><span style='background: #AEF1AE'>$(updater)</span><span style=''>
</span>209 <span style=''>
</span>210 <span style=''>  private def updaterValidator: String =&gt; Boolean = {
</span>211 <span style=''>    (value: String) =&gt; </span><span style='background: #AEF1AE'>value.split(&quot;,&quot;).map(ud =&gt; updaterValues.contains(ud.trim)).reduce(_ &amp;&amp; _)</span><span style=''>
</span>212 <span style=''>  }
</span>213 <span style=''>
</span>214 <span style=''>  private val updaterValues = </span><span style='background: #AEF1AE'>Array(&quot;grow_colmaker&quot;, &quot;distcol&quot;, &quot;grow_histmaker&quot;,
</span>215 <span style=''></span><span style='background: #AEF1AE'>    &quot;grow_local_histmaker&quot;, &quot;grow_skmaker&quot;, &quot;sync&quot;, &quot;refresh&quot;, &quot;prune&quot;)</span><span style=''>
</span>216 <span style=''>
</span>217 <span style=''>  /** A parameter of the 'refresh' updater plugin. When set to true, tree leaves and
</span>218 <span style=''>    * tree node stats are updated. When set to false, only tree node stats are updated.
</span>219 <span style=''>    * Default = 1
</span>220 <span style=''>    */
</span>221 <span style=''>  val refreshLeaf: IntParam = </span><span style='background: #AEF1AE'>new IntParam(this, &quot;refresh_leaf&quot;,
</span>222 <span style=''></span><span style='background: #AEF1AE'>    &quot;A parameter of the 'refresh' updater plugin. When this flag is true, tree leafs as &quot; +
</span>223 <span style=''></span><span style='background: #AEF1AE'>    &quot;well as tree nodes' stats are updated. When it is false, only node stats are updated.&quot;,
</span>224 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(0, 1)))</span><span style=''>
</span>225 <span style=''>  def getRefreshLeaf: Int = </span><span style='background: #AEF1AE'>$(refreshLeaf)</span><span style=''>
</span>226 <span style=''>
</span>227 <span style=''>  /** The type of boosting process to run. Can be default or update.
</span>228 <span style=''>    * Default = &quot;default&quot;
</span>229 <span style=''>    */
</span>230 <span style=''>  val processType: Param[String] = </span><span style='background: #AEF1AE'>new Param(this, &quot;process_type&quot;,
</span>231 <span style=''></span><span style='background: #AEF1AE'>    &quot;The type of boosting process to run. Can be default or update.&quot;,
</span>232 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(&quot;default&quot;, &quot;update&quot;)))</span><span style=''>
</span>233 <span style=''>  def getProcessType: String = </span><span style='background: #AEF1AE'>$(processType)</span><span style=''>
</span>234 <span style=''>
</span>235 <span style=''>  /** Controls the way that new nodes are added to the tree. Can be &quot;depthwise&quot; or &quot;lossguide&quot;.
</span>236 <span style=''>    * Currently supported only if tree_method is set to hist.
</span>237 <span style=''>    * Default = &quot;depthwise&quot;
</span>238 <span style=''>    */
</span>239 <span style=''>  val growPolicy: Param[String] = </span><span style='background: #AEF1AE'>new Param(this, &quot;grow_policy&quot;,
</span>240 <span style=''></span><span style='background: #AEF1AE'>    &quot;Controls the way new nodes are added to the tree. Can be 'depthwise' or 'lossguide'.&quot;,
</span>241 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(&quot;depthwise&quot;, &quot;lossguide&quot;)))</span><span style=''>
</span>242 <span style=''>  def getGrowPolicy: String = </span><span style='background: #AEF1AE'>$(growPolicy)</span><span style=''>
</span>243 <span style=''>
</span>244 <span style=''>  /** Maximum number of nodes to be added. Relevant only if grow_policy = lossguide. Must be &gt;= 0.
</span>245 <span style=''>    * Default = 0
</span>246 <span style=''>    */
</span>247 <span style=''>  val maxLeaves: IntParam = </span><span style='background: #AEF1AE'>new IntParam(this, &quot;max_leaves&quot;,
</span>248 <span style=''></span><span style='background: #AEF1AE'>    &quot;Maximum number of nodes to be added. Only relevant for the 'lossguide' grow policy. &quot; +
</span>249 <span style=''></span><span style='background: #AEF1AE'>      &quot;Must be &gt;= 0. &quot;,
</span>250 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.gtEq(0))</span><span style=''>
</span>251 <span style=''>  def getMaxLeaves: Int = </span><span style='background: #AEF1AE'>$(maxLeaves)</span><span style=''>
</span>252 <span style=''>
</span>253 <span style=''>  /** Maximum number of discrete bins to bucket continuous features. Used only if tree_method=hist.
</span>254 <span style=''>    * Default = 256
</span>255 <span style=''>    */
</span>256 <span style=''>  val maxBin: IntParam = </span><span style='background: #AEF1AE'>new IntParam(this, &quot;max_bin&quot;,
</span>257 <span style=''></span><span style='background: #AEF1AE'>    &quot;Maximum number of discrete bins to bucket continuous features. This is only used if &quot; +
</span>258 <span style=''></span><span style='background: #AEF1AE'>    &quot;'hist' is specified as tree_method. &quot;, ParamValidators.gtEq(1))</span><span style=''>
</span>259 <span style=''>  def getMaxBin: Int = </span><span style='background: #AEF1AE'>$(maxBin)</span><span style=''>
</span>260 <span style=''>
</span>261 <span style=''>  /* Dart Booster parameters */
</span>262 <span style=''>  /** Type of sampling algorithm. Can be &quot;uniform&quot; or &quot;weighted&quot;.
</span>263 <span style=''>    * Default = &quot;uniform&quot;
</span>264 <span style=''>    */
</span>265 <span style=''>  val sampleType: Param[String] = </span><span style='background: #AEF1AE'>new Param(this, &quot;sample_type&quot;,
</span>266 <span style=''></span><span style='background: #AEF1AE'>    &quot;Type of sampling algorithm. Can be 'uniform' or 'weighted'. &quot; +
</span>267 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;uniform\&quot;: dropped trees are selected uniformly.&quot; +
</span>268 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;weighted\&quot;: dropped trees are selected in proportion to weight.&quot;,
</span>269 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(&quot;uniform&quot;, &quot;weighted&quot;)))</span><span style=''>
</span>270 <span style=''>  def getSampleType: String = </span><span style='background: #AEF1AE'>$(sampleType)</span><span style=''>
</span>271 <span style=''>
</span>272 <span style=''>  /** Type of normalization algorithm. Can be &quot;tree&quot; or &quot;forest&quot;.
</span>273 <span style=''>    * Default = &quot;tree&quot;
</span>274 <span style=''>    */
</span>275 <span style=''>  val normalizeType: Param[String] = </span><span style='background: #AEF1AE'>new Param(this, &quot;normalize_type&quot;,
</span>276 <span style=''></span><span style='background: #AEF1AE'>    &quot;Type of normalization algorithm. Can be 'tree' or 'forest'.&quot; +
</span>277 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;tree\&quot;: new trees have the same weight of each of dropped trees.&quot; +
</span>278 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;forest\&quot;: new trees have the same weight of sum of dropped trees (forest).&quot;,
</span>279 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(&quot;tree&quot;, &quot;forest&quot;)))</span><span style=''>
</span>280 <span style=''>  def getNormalizeType: String = </span><span style='background: #AEF1AE'>$(normalizeType)</span><span style=''>
</span>281 <span style=''>
</span>282 <span style=''>  /** Dropout rate (a fraction of previous trees to drop during the dropout). Must be in [0, 1].
</span>283 <span style=''>    * Default = 0.0
</span>284 <span style=''>    */
</span>285 <span style=''>  val rateDrop: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;rate_drop&quot;,
</span>286 <span style=''></span><span style='background: #AEF1AE'>    &quot;dropout rate (a fraction of previous trees to drop during the dropout). Must be in [0, 1]. &quot;,
</span>287 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inRange(0.0, 1.0))</span><span style=''>
</span>288 <span style=''>  def getRateDrop: Double = </span><span style='background: #AEF1AE'>$(rateDrop)</span><span style=''>
</span>289 <span style=''>
</span>290 <span style=''>  /** Whether to drop at least one tree during the dropout.
</span>291 <span style=''>    * Default = 0
</span>292 <span style=''>    */
</span>293 <span style=''>  val oneDrop: IntParam = </span><span style='background: #AEF1AE'>new IntParam(this, &quot;one_drop&quot;,
</span>294 <span style=''></span><span style='background: #AEF1AE'>    &quot;whether to drop at least one tree during the dropout. &quot;,
</span>295 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(0, 1)))</span><span style=''>
</span>296 <span style=''>  def getOneDrop: Int = </span><span style='background: #AEF1AE'>$(oneDrop)</span><span style=''>
</span>297 <span style=''>
</span>298 <span style=''>  /** Probability of skipping the dropout procedure during a boosting iteration. Must be in [0, 1].
</span>299 <span style=''>    * Default: 0
</span>300 <span style=''>    */
</span>301 <span style=''>  val skipDrop: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;skip_drop&quot;,
</span>302 <span style=''></span><span style='background: #AEF1AE'>    &quot;Probability of skipping the dropout procedure during a boosting iteration. Must be in [0, 1].&quot;,
</span>303 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inRange(0.0, 1.0))</span><span style=''>
</span>304 <span style=''>  def getSkipDrop: Double = </span><span style='background: #AEF1AE'>$(skipDrop)</span><span style=''>
</span>305 <span style=''>
</span>306 <span style=''>  /* Parameters for linear booster */
</span>307 <span style=''>  /** L2 regularization term on bias. Must be in [0, 1].
</span>308 <span style=''>    * Default = 0.0
</span>309 <span style=''>    */
</span>310 <span style=''>  val lambdaBias: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;lambda_bias&quot;,
</span>311 <span style=''></span><span style='background: #AEF1AE'>    &quot;L2 regularization term on bias. Must be in [0, 1].&quot;, ParamValidators.inRange(0, 1))</span><span style=''>
</span>312 <span style=''>  def getLambdaBias: Double = </span><span style='background: #AEF1AE'>$(lambdaBias)</span><span style=''>
</span>313 <span style=''>
</span>314 <span style=''>
</span>315 <span style=''>  /* Parameters for Tweedie Regression */
</span>316 <span style=''>  /** Parameter that controls the variance of the Tweedie distribution. Must be in (1, 2).
</span>317 <span style=''>    * Default = 1.5
</span>318 <span style=''>    */
</span>319 <span style=''>  val tweedieVariancePower: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;tweedie_variance_power&quot;,
</span>320 <span style=''></span><span style='background: #AEF1AE'>    &quot;parameter that controls the variance of the Tweedie distribution. Must be in (1, 2).&quot;,
</span>321 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inRange(1, 2, lowerInclusive = false, upperInclusive = false))</span><span style=''>
</span>322 <span style=''>  def getTweedieVariancePower: Double = </span><span style='background: #AEF1AE'>$(tweedieVariancePower)</span><span style=''>
</span>323 <span style=''>
</span>324 <span style=''>  /* Learning task parameters */
</span>325 <span style=''>  /** Specifies the learning task and the corresponding learning objective.
</span>326 <span style=''>    * Default: &quot;reg:linear&quot;
</span>327 <span style=''>    */
</span>328 <span style=''>  val objective: Param[String] = </span><span style='background: #AEF1AE'>new Param(this, &quot;objective&quot;, &quot;Specifies the learning objective.&quot; +
</span>329 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;reg:linear\&quot; -- linear regression &quot; +
</span>330 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;reg:logistic\&quot; --logistic regression &quot; +
</span>331 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;binary:logistic\&quot; --logistic regression for binary classification, output is probability &quot; +
</span>332 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;binary:logitraw\&quot; --logistic regression for binary classification, output is score before&quot; +
</span>333 <span style=''></span><span style='background: #AEF1AE'>    &quot; logistic transformation &quot; +
</span>334 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;count:poisson\&quot; --poisson regression for count data, output mean of poisson distribution &quot; +
</span>335 <span style=''></span><span style='background: #AEF1AE'>    &quot;max_delta_step is set to 0.7 by default in poisson regression (used to safeguard &quot; +
</span>336 <span style=''></span><span style='background: #AEF1AE'>    &quot;optimization) &quot; +
</span>337 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;multi:softmax\&quot; --multiclass classification using the softmax objective. &quot; +
</span>338 <span style=''></span><span style='background: #AEF1AE'>    &quot;You also need to set num_class(number of classes)&quot; +
</span>339 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;multi:softprob\&quot; --same as softmax, but output a vector of ndata * nclass, which can be&quot; +
</span>340 <span style=''></span><span style='background: #AEF1AE'>    &quot; further reshaped to ndata, nclass matrix. The result contains predicted probability of each&quot; +
</span>341 <span style=''></span><span style='background: #AEF1AE'>    &quot; data point belonging to each class. &quot; +
</span>342 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;rank:pairwise\&quot; --set XGBoost to do ranking task by minimizing the pairwise loss &quot; +
</span>343 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;reg:gamma\&quot; --gamma regression with log-link. Output is a mean of gamma distribution. &quot; +
</span>344 <span style=''></span><span style='background: #AEF1AE'>    &quot;It might be useful, e.g., for modeling insurance claims severity, or for any outcome &quot; +
</span>345 <span style=''></span><span style='background: #AEF1AE'>    &quot;that might be gamma-distributed&quot; +
</span>346 <span style=''></span><span style='background: #AEF1AE'>    &quot;\&quot;reg:tweedie\&quot; --Tweedie regression with log-link. It might be useful, e.g., for &quot; +
</span>347 <span style=''></span><span style='background: #AEF1AE'>    &quot;modeling total loss in insurance, or for any outcome that might be Tweedie-distributed.&quot;,
</span>348 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(&quot;reg:linear&quot;, &quot;reg:logistic&quot;, &quot;binary:logistic&quot;,
</span>349 <span style=''></span><span style='background: #AEF1AE'>      &quot;binary:logistraw&quot;, &quot;count:poisson&quot;, &quot;multi:softmax&quot;, &quot;multi:softprob&quot;,
</span>350 <span style=''></span><span style='background: #AEF1AE'>      &quot;rank:pairwise&quot;, &quot;reg:gamma&quot;, &quot;reg:tweedie&quot;)))</span><span style=''>
</span>351 <span style=''>  def getObjective: String = </span><span style='background: #AEF1AE'>$(objective)</span><span style=''>
</span>352 <span style=''>
</span>353 <span style=''>  /**
</span>354 <span style=''>    * No default. Used for softmax multiclass classification.
</span>355 <span style=''>    */
</span>356 <span style=''>  val numClasses: IntParam = </span><span style='background: #AEF1AE'>new IntParam(this, &quot;num_class&quot;,
</span>357 <span style=''></span><span style='background: #AEF1AE'>    &quot;Number of classes&quot;, ParamValidators.gtEq(1))</span><span style=''>
</span>358 <span style=''>  def getNumClasses: Int = </span><span style='background: #AEF1AE'>$(numClasses)</span><span style=''>
</span>359 <span style=''>
</span>360 <span style=''>  /** The initial prediction score of all instances, global bias.
</span>361 <span style=''>    * Default = 0.5
</span>362 <span style=''>    */
</span>363 <span style=''>  val baseScore: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;base_score&quot;,
</span>364 <span style=''></span><span style='background: #AEF1AE'>    &quot;the initial prediction score of all instances, global bias&quot;)</span><span style=''>
</span>365 <span style=''>  def getBaseScore: Double = </span><span style='background: #AEF1AE'>$(baseScore)</span><span style=''>
</span>366 <span style=''>
</span>367 <span style=''>  /** Evaluation metrics for validation data. A default metric will be assigned according to the
</span>368 <span style=''>    * objective (rmse for regression, error for classification, and map for ranking )
</span>369 <span style=''>    * Default according to objective
</span>370 <span style=''>    */
</span>371 <span style=''>  val evalMetric: Param[String] = </span><span style='background: #AEF1AE'>new Param(this, &quot;eval_metric&quot;,
</span>372 <span style=''></span><span style='background: #AEF1AE'>    &quot;Evaluation metrics for validation data. A default metric will be assigned according to &quot; +
</span>373 <span style=''></span><span style='background: #AEF1AE'>    &quot;objective (rmse for regression, and error for classification, mean average &quot; +
</span>374 <span style=''></span><span style='background: #AEF1AE'>    &quot;precision for ranking)&quot;,
</span>375 <span style=''></span><span style='background: #AEF1AE'>    ParamValidators.inArray(Array(&quot;rmse&quot;, &quot;mae&quot;, &quot;logloss&quot;, &quot;error&quot;, &quot;error@t&quot;, &quot;merror&quot;,
</span>376 <span style=''></span><span style='background: #AEF1AE'>    &quot;mlogloss&quot;, &quot;auc&quot;, &quot;ndcg&quot;, &quot;map&quot;, &quot;ndcg@n&quot;, &quot;ndcg-&quot;, &quot;ndcg@n-&quot;, &quot;map-&quot;, &quot;map@n-&quot;)))</span><span style=''>
</span>377 <span style=''>  def getEvalMetric: String = </span><span style='background: #AEF1AE'>$(evalMetric)</span><span style=''>
</span>378 <span style=''>
</span>379 <span style=''>  /** Random number seed.
</span>380 <span style=''>    * Default = 0
</span>381 <span style=''>    */
</span>382 <span style=''>  val seed: DoubleParam = </span><span style='background: #AEF1AE'>new DoubleParam(this, &quot;seed&quot;, &quot;Random number seed.&quot;)</span><span style=''>
</span>383 <span style=''>  def getSeed: Double = </span><span style='background: #AEF1AE'>$(seed)</span><span style=''>
</span>384 <span style=''>
</span>385 <span style=''>  /**
</span>386 <span style=''>    * Number of rounds for gradient boosting. Must be &gt;= 1. Required.
</span>387 <span style=''>    */
</span>388 <span style=''>  val numRound: IntParam = </span><span style='background: #AEF1AE'>new IntParam(this, &quot;num_round&quot;,
</span>389 <span style=''></span><span style='background: #AEF1AE'>    &quot;Number of rounds. Must be &gt;= 1. &quot;, ParamValidators.gtEq(1))</span><span style=''>
</span>390 <span style=''>  def getNumRound: Int = </span><span style='background: #AEF1AE'>$(numRound)</span><span style=''>
</span>391 <span style=''>
</span>392 <span style=''>}
</span>393 <span style=''>
</span>394 <span style=''>object XGBoostSageMakerEstimator {
</span>395 <span style=''>  val algorithmName = </span><span style='background: #AEF1AE'>&quot;xgboost&quot;</span><span style=''>
</span>396 <span style=''>  val algorithmTag = </span><span style='background: #AEF1AE'>&quot;1&quot;</span><span style=''>
</span>397 <span style=''>  val regionAccountMap = </span><span style='background: #AEF1AE'>SagerMakerRegionAccountMaps.ApplicationsAccountMap</span><span style=''>
</span>398 <span style=''>}
</span>399 <span style=''>
</span>400 <span style=''>/**
</span>401 <span style=''>  * A [[SageMakerEstimator]] that runs an XGBoost training job in SageMaker and returns a
</span>402 <span style=''>  * [[SageMakerModel]] that can be used to transform a DataFrame using
</span>403 <span style=''>  * the hosted XGBoost model. XGBoost is an open-source distributed gradient boosting
</span>404 <span style=''>  * library that Amazon SageMaker has adapted to run on Amazon SageMaker.
</span>405 <span style=''>  *
</span>406 <span style=''>  * XGBoost trains and infers on LibSVM-formatted data. XGBoostSageMakerEstimator uses
</span>407 <span style=''>  * Spark's LibSVMFileFormat to write the training DataFrame to S3, and serializes
</span>408 <span style=''>  * Rows to LibSVM for inference, selecting the column named &quot;features&quot; by default,
</span>409 <span style=''>  * expected to contain a Vector of Doubles.
</span>410 <span style=''>  *
</span>411 <span style=''>  * Inferences made against an Endpoint hosting an XGBoost model contain
</span>412 <span style=''>  * a &quot;prediction&quot; field appended to the input DataFrame as a column of Doubles, containing
</span>413 <span style=''>  * the prediction corresponding to the given Vector of features.
</span>414 <span style=''>  *
</span>415 <span style=''>  * @see [[https://github.com/dmlc/xgboost]] for more on XGBoost.
</span>416 <span style=''>  *
</span>417 <span style=''>  * @param sagemakerRole The SageMaker TrainingJob and Hosting IAM Role. Used by a SageMaker to
</span>418 <span style=''>  *                      access S3 and ECR resources. SageMaker hosted Endpoints instances
</span>419 <span style=''>  *                      launched by this Estimator run with this role.
</span>420 <span style=''>  * @param requestRowSerializer Serializes Spark DataFrame [[Row]]s for transformation by
</span>421 <span style=''>  *                             Models built from this Estimator.
</span>422 <span style=''>  * @param responseRowDeserializer Deserializes an Endpoint response into a series of [[Row]]s.
</span>423 <span style=''>  * @param trainingInputS3DataPath An S3 location to upload SageMaker Training Job input data to.
</span>424 <span style=''>  * @param trainingOutputS3DataPath An S3 location for SageMaker to store Training Job output
</span>425 <span style=''>  *                                 data to.
</span>426 <span style=''>  * @param trainingInstanceType The SageMaker TrainingJob Instance Type to use.
</span>427 <span style=''>  * @param trainingInstanceCount The number of instances of instanceType to run a SageMaker
</span>428 <span style=''>  *                              Training Job with.
</span>429 <span style=''>  * @param trainingInstanceVolumeSizeInGB The EBS volume size in gigabytes of each instance
</span>430 <span style=''>  * @param trainingProjectedColumns The columns to project from the Dataset being fit before
</span>431 <span style=''>  *                                 training. If an Optional.empty is passed then no specific
</span>432 <span style=''>  *                                 projection will occur and all columns will be serialized.
</span>433 <span style=''>  * @param trainingChannelName The SageMaker Channel name to input serialized Dataset fit input to
</span>434 <span style=''>  * @param trainingContentType The MIME type of the training data.
</span>435 <span style=''>  * @param trainingS3DataDistribution The SageMaker Training Job S3 data distribution scheme.
</span>436 <span style=''>  * @param trainingSparkDataFormat The Spark Data Format name used to serialize the Dataset being
</span>437 <span style=''>  *                                fit for input to SageMaker.
</span>438 <span style=''>  * @param trainingSparkDataFormatOptions The Spark Data Format Options used during serialization
</span>439 <span style=''>  *                                       of the Dataset being fit.
</span>440 <span style=''>  * @param trainingInputMode The SageMaker Training Job Channel input mode.
</span>441 <span style=''>  * @param trainingCompressionCodec The type of compression to use when serializing the Dataset
</span>442 <span style=''>  *                                 being fit for input to SageMaker.
</span>443 <span style=''>  * @param trainingMaxRuntimeInSeconds A SageMaker Training Job Termination Condition
</span>444 <span style=''>  *                                    MaxRuntimeInHours.
</span>445 <span style=''>  * @param trainingKmsKeyId A KMS key ID for the Output Data Source
</span>446 <span style=''>  * @param modelEnvironmentVariables The environment variables that SageMaker will set on the model
</span>447 <span style=''>  *                                  container during execution.
</span>448 <span style=''>  * @param endpointInstanceType The SageMaker Endpoint Confing instance type.
</span>449 <span style=''>  * @param endpointInitialInstanceCount The SageMaker Endpoint Config minimum number of instances
</span>450 <span style=''>  *                                     that can be used to host modelImage.
</span>451 <span style=''>  * @param endpointCreationPolicy Defines how a SageMaker Endpoint referenced by a
</span>452 <span style=''>  *                               SageMakerModel is created.
</span>453 <span style=''>  * @param sagemakerClient Amazon SageMaker client. Used to send CreateTrainingJob, CreateModel,
</span>454 <span style=''>  *                        and CreateEndpoint requests.
</span>455 <span style=''>  * @param region The region in which to run the algorithm. If not specified, gets the region from
</span>456 <span style=''>  *               the DefaultAwsRegionProviderChain.
</span>457 <span style=''>  * @param s3Client AmazonS3. Used to create a bucket for staging SageMaker Training Job input
</span>458 <span style=''>  *                 and/or output if either are set to S3AutoCreatePath.
</span>459 <span style=''>  * @param stsClient AmazonSTS. Used to resolve the account number when creating staging
</span>460 <span style=''>  *                  input / output buckets.
</span>461 <span style=''>  * @param modelPrependInputRowsToTransformationRows Whether the transformation result on Models
</span>462 <span style=''>  *        built by this Estimator should also include the input Rows. If true, each output Row
</span>463 <span style=''>  *        is formed by a concatenation of the input Row with the corresponding Row produced by
</span>464 <span style=''>  *        SageMaker Endpoint invocation, produced by responseRowDeserializer.
</span>465 <span style=''>  *        If false, each output Row is just taken from responseRowDeserializer.
</span>466 <span style=''>  * @param deleteStagingDataAfterTraining Whether to remove the training data on s3 after training
</span>467 <span style=''>  *                                       is complete or failed.
</span>468 <span style=''>  * @param namePolicyFactory The [[NamePolicyFactory]] to use when naming SageMaker entities
</span>469 <span style=''>  *        created during fit
</span>470 <span style=''>  * @param uid The unique identifier of this Estimator. Used to represent this stage in
</span>471 <span style=''>  *            Spark ML pipelines.
</span>472 <span style=''>  */
</span>473 <span style=''>class XGBoostSageMakerEstimator(
</span>474 <span style=''>           override val sagemakerRole : IAMRoleResource = IAMRoleFromConfig(),
</span>475 <span style=''>           override val trainingInstanceType : String,
</span>476 <span style=''>           override val trainingInstanceCount : Int,
</span>477 <span style=''>           override val endpointInstanceType : String,
</span>478 <span style=''>           override val endpointInitialInstanceCount : Int,
</span>479 <span style=''>           override val requestRowSerializer : RequestRowSerializer =
</span>480 <span style=''>             new LibSVMRequestRowSerializer(),
</span>481 <span style=''>           override val responseRowDeserializer : ResponseRowDeserializer =
</span>482 <span style=''>             new XGBoostCSVRowDeserializer(),
</span>483 <span style=''>           override val trainingInputS3DataPath : S3Resource = S3AutoCreatePath(),
</span>484 <span style=''>           override val trainingOutputS3DataPath : S3Resource = S3AutoCreatePath(),
</span>485 <span style=''>           override val trainingInstanceVolumeSizeInGB : Int = 1024,
</span>486 <span style=''>           override val trainingProjectedColumns : Option[List[String]] = None,
</span>487 <span style=''>           override val trainingChannelName : String = &quot;train&quot;,
</span>488 <span style=''>           override val trainingContentType: Option[String] = Some(&quot;libsvm&quot;),
</span>489 <span style=''>           override val trainingS3DataDistribution : String =
</span>490 <span style=''>             S3DataDistribution.ShardedByS3Key.toString,
</span>491 <span style=''>           override val trainingSparkDataFormat : String = &quot;libsvm&quot;,
</span>492 <span style=''>           override val trainingSparkDataFormatOptions : Map[String, String] =
</span>493 <span style=''>             Map(),
</span>494 <span style=''>           override val trainingInputMode : String =
</span>495 <span style=''>             TrainingInputMode.File.toString,
</span>496 <span style=''>           override val trainingCompressionCodec : Option[String] = None,
</span>497 <span style=''>           override val trainingMaxRuntimeInSeconds : Int = 24 * 60 * 60,
</span>498 <span style=''>           override val trainingKmsKeyId : Option[String] = None,
</span>499 <span style=''>           override val modelEnvironmentVariables : Map[String, String] = Map(),
</span>500 <span style=''>           override val endpointCreationPolicy : EndpointCreationPolicy =
</span>501 <span style=''>             EndpointCreationPolicy.CREATE_ON_CONSTRUCT,
</span>502 <span style=''>           override val sagemakerClient : AmazonSageMaker
</span>503 <span style=''>             = AmazonSageMakerClientBuilder.defaultClient,
</span>504 <span style=''>           val region : Option[String] = None,
</span>505 <span style=''>           override val s3Client : AmazonS3 = AmazonS3ClientBuilder.defaultClient(),
</span>506 <span style=''>           override val stsClient : AWSSecurityTokenService =
</span>507 <span style=''>             AWSSecurityTokenServiceClientBuilder.defaultClient(),
</span>508 <span style=''>           override val modelPrependInputRowsToTransformationRows : Boolean = true,
</span>509 <span style=''>           override val deleteStagingDataAfterTraining : Boolean = true,
</span>510 <span style=''>           override val namePolicyFactory : NamePolicyFactory = new RandomNamePolicyFactory(),
</span>511 <span style=''>           override val uid : String = Identifiable.randomUID(&quot;sagemaker&quot;))
</span>512 <span style=''>  extends SageMakerEstimator(
</span>513 <span style=''>    trainingImage = SageMakerImageURIProvider.getImage(
</span>514 <span style=''>      region.getOrElse(new DefaultAwsRegionProviderChain().getRegion),
</span>515 <span style=''>      XGBoostSageMakerEstimator.regionAccountMap,
</span>516 <span style=''>      XGBoostSageMakerEstimator.algorithmName, XGBoostSageMakerEstimator.algorithmTag),
</span>517 <span style=''>    modelImage = SageMakerImageURIProvider.getImage(
</span>518 <span style=''>      region.getOrElse(new DefaultAwsRegionProviderChain().getRegion),
</span>519 <span style=''>      XGBoostSageMakerEstimator.regionAccountMap,
</span>520 <span style=''>      XGBoostSageMakerEstimator.algorithmName, XGBoostSageMakerEstimator.algorithmTag),
</span>521 <span style=''>    sagemakerRole,
</span>522 <span style=''>    trainingInstanceType,
</span>523 <span style=''>    trainingInstanceCount,
</span>524 <span style=''>    endpointInstanceType,
</span>525 <span style=''>    endpointInitialInstanceCount,
</span>526 <span style=''>    requestRowSerializer,
</span>527 <span style=''>    responseRowDeserializer,
</span>528 <span style=''>    trainingInputS3DataPath,
</span>529 <span style=''>    trainingOutputS3DataPath,
</span>530 <span style=''>    trainingInstanceVolumeSizeInGB,
</span>531 <span style=''>    trainingProjectedColumns,
</span>532 <span style=''>    trainingChannelName,
</span>533 <span style=''>    trainingContentType,
</span>534 <span style=''>    trainingS3DataDistribution,
</span>535 <span style=''>    trainingSparkDataFormat,
</span>536 <span style=''>    trainingSparkDataFormatOptions,
</span>537 <span style=''>    trainingInputMode,
</span>538 <span style=''>    trainingCompressionCodec,
</span>539 <span style=''>    trainingMaxRuntimeInSeconds,
</span>540 <span style=''>    trainingKmsKeyId,
</span>541 <span style=''>    modelEnvironmentVariables,
</span>542 <span style=''>    endpointCreationPolicy,
</span>543 <span style=''>    sagemakerClient,
</span>544 <span style=''>    s3Client,
</span>545 <span style=''>    stsClient,
</span>546 <span style=''>    modelPrependInputRowsToTransformationRows,
</span>547 <span style=''>    deleteStagingDataAfterTraining,
</span>548 <span style=''>    namePolicyFactory,
</span>549 <span style=''>    uid) with XGBoostParams {
</span>550 <span style=''>
</span>551 <span style=''>  def setBooster(value: String) : this.type = </span><span style='background: #AEF1AE'>set(booster, value)</span><span style=''>
</span>552 <span style=''>
</span>553 <span style=''>  def setSilent(value: Int) : this.type = </span><span style='background: #AEF1AE'>set(silent, value)</span><span style=''>
</span>554 <span style=''>
</span>555 <span style=''>  def setNThread(value: Int) : this.type = </span><span style='background: #AEF1AE'>set(nThread, value)</span><span style=''>
</span>556 <span style=''>
</span>557 <span style=''>  def setEta(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(eta, value)</span><span style=''>
</span>558 <span style=''>
</span>559 <span style=''>  def setGamma(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(gamma, value)</span><span style=''>
</span>560 <span style=''>
</span>561 <span style=''>  def setMaxDepth(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(maxDepth, value)</span><span style=''>
</span>562 <span style=''>
</span>563 <span style=''>  def setMinChildWeight(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(minChildWeight, value)</span><span style=''>
</span>564 <span style=''>
</span>565 <span style=''>  def setMaxDeltaStep(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(maxDeltaStep, value)</span><span style=''>
</span>566 <span style=''>
</span>567 <span style=''>  def setSubsample(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(subsample, value)</span><span style=''>
</span>568 <span style=''>
</span>569 <span style=''>  def setColSampleByTree(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(colSampleByTree, value)</span><span style=''>
</span>570 <span style=''>
</span>571 <span style=''>  def setColSampleByLevel(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(colSampleByLevel, value)</span><span style=''>
</span>572 <span style=''>
</span>573 <span style=''>  def setLambda(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(lambda, value)</span><span style=''>
</span>574 <span style=''>
</span>575 <span style=''>  def setAlpha(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(alpha, value)</span><span style=''>
</span>576 <span style=''>
</span>577 <span style=''>  def setTreeMethod(value: String) : this.type = </span><span style='background: #AEF1AE'>set(treeMethod, value)</span><span style=''>
</span>578 <span style=''>
</span>579 <span style=''>  def setSketchEps(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(sketchEps, value)</span><span style=''>
</span>580 <span style=''>
</span>581 <span style=''>  def setScalePosWeight(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(scalePosWeight, value)</span><span style=''>
</span>582 <span style=''>
</span>583 <span style=''>  def setUpdater(value: String) : this.type = </span><span style='background: #AEF1AE'>set(updater, value)</span><span style=''>
</span>584 <span style=''>
</span>585 <span style=''>  def setRefreshLeaf(value: Int) : this.type = </span><span style='background: #AEF1AE'>set(refreshLeaf, value)</span><span style=''>
</span>586 <span style=''>
</span>587 <span style=''>  def setProcessType(value: String) : this.type = </span><span style='background: #AEF1AE'>set(processType, value)</span><span style=''>
</span>588 <span style=''>
</span>589 <span style=''>  def setGrowPolicy(value: String) : this.type = </span><span style='background: #AEF1AE'>set(growPolicy, value)</span><span style=''>
</span>590 <span style=''>
</span>591 <span style=''>  def setMaxLeaves(value: Int) : this.type = </span><span style='background: #AEF1AE'>set(maxLeaves, value)</span><span style=''>
</span>592 <span style=''>
</span>593 <span style=''>  def setMaxBin(value: Int) : this.type = </span><span style='background: #AEF1AE'>set(maxBin, value)</span><span style=''>
</span>594 <span style=''>
</span>595 <span style=''>  def setSampleType(value: String) : this.type = </span><span style='background: #AEF1AE'>set(sampleType, value)</span><span style=''>
</span>596 <span style=''>
</span>597 <span style=''>  def setNormalizeType(value: String) : this.type = </span><span style='background: #AEF1AE'>set(normalizeType, value)</span><span style=''>
</span>598 <span style=''>
</span>599 <span style=''>  def setRateDrop(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(rateDrop, value)</span><span style=''>
</span>600 <span style=''>
</span>601 <span style=''>  def setOneDrop(value: Int) : this.type = </span><span style='background: #AEF1AE'>set(oneDrop, value)</span><span style=''>
</span>602 <span style=''>
</span>603 <span style=''>  def setSkipDrop(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(skipDrop, value)</span><span style=''>
</span>604 <span style=''>
</span>605 <span style=''>  def setLambdaBias(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(lambdaBias, value)</span><span style=''>
</span>606 <span style=''>
</span>607 <span style=''>  def setTweedieVariancePower(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(tweedieVariancePower, value)</span><span style=''>
</span>608 <span style=''>
</span>609 <span style=''>  def setObjective(value: String) : this.type = </span><span style='background: #AEF1AE'>set(objective, value)</span><span style=''>
</span>610 <span style=''>
</span>611 <span style=''>  def setNumClasses(value: Int) : this.type = </span><span style='background: #AEF1AE'>set(numClasses, value)</span><span style=''>
</span>612 <span style=''>
</span>613 <span style=''>  def setBaseScore(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(baseScore, value)</span><span style=''>
</span>614 <span style=''>
</span>615 <span style=''>  def setEvalMetric(value: String) : this.type = </span><span style='background: #AEF1AE'>set(evalMetric, value)</span><span style=''>
</span>616 <span style=''>
</span>617 <span style=''>  def setSeed(value: Double) : this.type = </span><span style='background: #AEF1AE'>set(seed, value)</span><span style=''>
</span>618 <span style=''>
</span>619 <span style=''>  def setNumRound(value: Int) : this.type = </span><span style='background: #AEF1AE'>set(numRound, value)</span><span style=''>
</span>620 <span style=''>
</span>621 <span style=''>  // Check whether required hyper-parameters are set
</span>622 <span style=''>  override def transformSchema(schema: StructType): StructType = {
</span>623 <span style=''>    </span><span style='background: #F0ADAD'>$(numRound)</span><span style=''>
</span>624 <span style=''>    </span><span style='background: #F0ADAD'>super.transformSchema(schema)</span><span style=''>
</span>625 <span style=''>  }
</span>626 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Code</th>
      </tr><tr>
        <td>
          42
        </td>
        <td>
          1123
        </td>
        <td>
          1997
          -
          2006
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;booster&quot;
        </td>
      </tr><tr>
        <td>
          42
        </td>
        <td>
          1130
        </td>
        <td>
          1981
          -
          2218
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Param.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.Param[String](this, &quot;booster&quot;, &quot;Which booster to use. Can be gbtree, gblinear or dart. gbtree and dart use tree based model while gblinear uses linear function.&quot;, org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))))
        </td>
      </tr><tr>
        <td>
          43
        </td>
        <td>
          1124
        </td>
        <td>
          2012
          -
          2151
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Which booster to use. Can be gbtree, gblinear or dart. gbtree and dart use tree based model while gblinear uses linear function.&quot;
        </td>
      </tr><tr>
        <td>
          45
        </td>
        <td>
          1127
        </td>
        <td>
          2209
          -
          2215
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;dart&quot;
        </td>
      </tr><tr>
        <td>
          45
        </td>
        <td>
          1126
        </td>
        <td>
          2197
          -
          2207
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;gblinear&quot;
        </td>
      </tr><tr>
        <td>
          45
        </td>
        <td>
          1129
        </td>
        <td>
          2157
          -
          2217
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          45
        </td>
        <td>
          1125
        </td>
        <td>
          2187
          -
          2195
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;gbtree&quot;
        </td>
      </tr><tr>
        <td>
          45
        </td>
        <td>
          1128
        </td>
        <td>
          2181
          -
          2216
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;gbtree&quot;, &quot;gblinear&quot;, &quot;dart&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          46
        </td>
        <td>
          1132
        </td>
        <td>
          2246
          -
          2256
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[String](XGBoostParams.this.booster)
        </td>
      </tr><tr>
        <td>
          46
        </td>
        <td>
          1131
        </td>
        <td>
          2248
          -
          2255
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.booster
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.booster
        </td>
      </tr><tr>
        <td>
          52
        </td>
        <td>
          1133
        </td>
        <td>
          2435
          -
          2443
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;silent&quot;
        </td>
      </tr><tr>
        <td>
          52
        </td>
        <td>
          1137
        </td>
        <td>
          2416
          -
          2594
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.IntParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.IntParam(this, &quot;silent&quot;, &quot;Whether in silent mode. Can be 0 or 1. 0 means print running messages, 1 means silent mode.&quot;, org.apache.spark.ml.param.ParamValidators.inArray[Int](scala.Array.apply(0, 1)))
        </td>
      </tr><tr>
        <td>
          53
        </td>
        <td>
          1134
        </td>
        <td>
          2449
          -
          2551
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Whether in silent mode. Can be 0 or 1. 0 means print running messages, 1 means silent mode.&quot;
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          1136
        </td>
        <td>
          2557
          -
          2593
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[Int](scala.Array.apply(0, 1))
        </td>
      </tr><tr>
        <td>
          55
        </td>
        <td>
          1135
        </td>
        <td>
          2581
          -
          2592
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply(0, 1)
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          1138
        </td>
        <td>
          2620
          -
          2626
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.silent
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.silent
        </td>
      </tr><tr>
        <td>
          56
        </td>
        <td>
          1139
        </td>
        <td>
          2618
          -
          2627
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Int](XGBoostParams.this.silent)
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          1140
        </td>
        <td>
          2804
          -
          2813
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;nthread&quot;
        </td>
      </tr><tr>
        <td>
          61
        </td>
        <td>
          1143
        </td>
        <td>
          2785
          -
          2913
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.IntParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.IntParam(this, &quot;nthread&quot;, &quot;Number of parallel threads used to run xgboost. Must be &gt;= 1. &quot;, org.apache.spark.ml.param.ParamValidators.gtEq[Any](1.0))
        </td>
      </tr><tr>
        <td>
          62
        </td>
        <td>
          1141
        </td>
        <td>
          2819
          -
          2883
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Number of parallel threads used to run xgboost. Must be &gt;= 1. &quot;
        </td>
      </tr><tr>
        <td>
          63
        </td>
        <td>
          1142
        </td>
        <td>
          2889
          -
          2912
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.gtEq
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.gtEq[Any](1.0)
        </td>
      </tr><tr>
        <td>
          64
        </td>
        <td>
          1145
        </td>
        <td>
          2938
          -
          2948
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Int](XGBoostParams.this.nThread)
        </td>
      </tr><tr>
        <td>
          64
        </td>
        <td>
          1144
        </td>
        <td>
          2940
          -
          2947
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.nThread
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.nThread
        </td>
      </tr><tr>
        <td>
          72
        </td>
        <td>
          1149
        </td>
        <td>
          3291
          -
          3617
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;eta&quot;, &quot;Step size shrinkage used in update to prevent overfitting. After each boosting step, we can directly get the weights of new features. and eta shrinks the feature weights to make the boosting process more conservative. Must be in [0, 1]. &quot;, org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0))
        </td>
      </tr><tr>
        <td>
          72
        </td>
        <td>
          1146
        </td>
        <td>
          3313
          -
          3318
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;eta&quot;
        </td>
      </tr><tr>
        <td>
          74
        </td>
        <td>
          1147
        </td>
        <td>
          3324
          -
          3581
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Step size shrinkage used in update to prevent overfitting. After each boosting step, we can directly get the weights of new features. and eta shrinks the feature weights to make the boosting process more conservative. Must be in [0, 1]. &quot;
        </td>
      </tr><tr>
        <td>
          76
        </td>
        <td>
          1148
        </td>
        <td>
          3587
          -
          3616
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inRange
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0)
        </td>
      </tr><tr>
        <td>
          77
        </td>
        <td>
          1151
        </td>
        <td>
          3641
          -
          3647
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.eta)
        </td>
      </tr><tr>
        <td>
          77
        </td>
        <td>
          1150
        </td>
        <td>
          3643
          -
          3646
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.eta
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.eta
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          1152
        </td>
        <td>
          3894
          -
          3901
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;gamma&quot;
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          1155
        </td>
        <td>
          3872
          -
          4120
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;gamma&quot;, &quot;Minimum loss reduction required to make an additional partition on a leaf node of the tree. The larger the value, the more conservative the algorithm will be. Must be &gt;= 0.&quot;, org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0))
        </td>
      </tr><tr>
        <td>
          84
        </td>
        <td>
          1153
        </td>
        <td>
          3907
          -
          4090
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Minimum loss reduction required to make an additional partition on a leaf node of the tree. The larger the value, the more conservative the algorithm will be. Must be &gt;= 0.&quot;
        </td>
      </tr><tr>
        <td>
          86
        </td>
        <td>
          1154
        </td>
        <td>
          4096
          -
          4119
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.gtEq
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0)
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          1156
        </td>
        <td>
          4148
          -
          4153
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.gamma
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.gamma
        </td>
      </tr><tr>
        <td>
          87
        </td>
        <td>
          1157
        </td>
        <td>
          4146
          -
          4154
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.gamma)
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          1158
        </td>
        <td>
          4440
          -
          4451
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;max_depth&quot;
        </td>
      </tr><tr>
        <td>
          94
        </td>
        <td>
          1161
        </td>
        <td>
          4418
          -
          4701
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;max_depth&quot;, &quot; Maximum depth of a tree, increase this value will make the model more complex (likely to be overfitting). 0 indicates no limit, limit is required when grow_policy=depth-wise. Must be &gt;= 0. &quot;, org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0))
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          1159
        </td>
        <td>
          4457
          -
          4671
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot; Maximum depth of a tree, increase this value will make the model more complex (likely to be overfitting). 0 indicates no limit, limit is required when grow_policy=depth-wise. Must be &gt;= 0. &quot;
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          1160
        </td>
        <td>
          4677
          -
          4700
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.gtEq
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0)
        </td>
      </tr><tr>
        <td>
          99
        </td>
        <td>
          1162
        </td>
        <td>
          4732
          -
          4740
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.maxDepth
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.maxDepth
        </td>
      </tr><tr>
        <td>
          99
        </td>
        <td>
          1163
        </td>
        <td>
          4730
          -
          4741
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.maxDepth)
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          1167
        </td>
        <td>
          5251
          -
          5781
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;min_child_weight&quot;, &quot;Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger the value, the more conservative the algorithm will be. Must be &gt;= 0.&quot;, org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0))
        </td>
      </tr><tr>
        <td>
          108
        </td>
        <td>
          1164
        </td>
        <td>
          5273
          -
          5291
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;min_child_weight&quot;
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          1165
        </td>
        <td>
          5297
          -
          5755
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. In linear regression mode, this simply corresponds to minimum number of instances needed to be in each node. The larger the value, the more conservative the algorithm will be. Must be &gt;= 0.&quot;
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          1166
        </td>
        <td>
          5757
          -
          5780
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.gtEq
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0)
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1169
        </td>
        <td>
          5816
          -
          5833
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.minChildWeight)
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1168
        </td>
        <td>
          5818
          -
          5832
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.minChildWeight
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.minChildWeight
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          1170
        </td>
        <td>
          6210
          -
          6226
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;max_delta_step&quot;
        </td>
      </tr><tr>
        <td>
          121
        </td>
        <td>
          1173
        </td>
        <td>
          6188
          -
          6703
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;max_delta_step&quot;, &quot;Maximum delta step allowed for each tree\'s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help make the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when the classes are extremely imbalanced. Setting it to value of 1-10 might help control the update. Must be &gt;= 0.&quot;, org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0))
        </td>
      </tr><tr>
        <td>
          125
        </td>
        <td>
          1171
        </td>
        <td>
          6232
          -
          6677
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Maximum delta step allowed for each tree\'s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help make the update step more conservative. Usually this parameter is not needed, but it might help in logistic regression when the classes are extremely imbalanced. Setting it to value of 1-10 might help control the update. Must be &gt;= 0.&quot;
        </td>
      </tr><tr>
        <td>
          126
        </td>
        <td>
          1172
        </td>
        <td>
          6679
          -
          6702
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.gtEq
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0)
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          1174
        </td>
        <td>
          6738
          -
          6750
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.maxDeltaStep
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.maxDeltaStep
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          1175
        </td>
        <td>
          6736
          -
          6751
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.maxDeltaStep)
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          1176
        </td>
        <td>
          7042
          -
          7053
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;subsample&quot;
        </td>
      </tr><tr>
        <td>
          134
        </td>
        <td>
          1179
        </td>
        <td>
          7020
          -
          7359
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;subsample&quot;, &quot;Subsample ratio of the training instance. Setting it to 0.5 means that XGBoost will randomly collect half of the data instances to grow trees and this will prevent overfitting.Must be in (0, 1]. &quot;, org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0, false, true))
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          1177
        </td>
        <td>
          7059
          -
          7276
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Subsample ratio of the training instance. Setting it to 0.5 means that XGBoost will randomly collect half of the data instances to grow trees and this will prevent overfitting.Must be in (0, 1]. &quot;
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1178
        </td>
        <td>
          7282
          -
          7358
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inRange
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0, false, true)
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1180
        </td>
        <td>
          7391
          -
          7400
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.subsample
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.subsample
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1181
        </td>
        <td>
          7389
          -
          7401
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.subsample)
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          1185
        </td>
        <td>
          7546
          -
          7750
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;colsample_bytree&quot;, &quot;Subsample ratio of columns when constructing each tree. Must be in (0, 1]&quot;, org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0, false, true))
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          1182
        </td>
        <td>
          7568
          -
          7586
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;colsample_bytree&quot;
        </td>
      </tr><tr>
        <td>
          146
        </td>
        <td>
          1183
        </td>
        <td>
          7592
          -
          7667
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Subsample ratio of columns when constructing each tree. Must be in (0, 1]&quot;
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          1184
        </td>
        <td>
          7673
          -
          7749
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inRange
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0, false, true)
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          1187
        </td>
        <td>
          7786
          -
          7804
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.colSampleByTree)
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          1186
        </td>
        <td>
          7788
          -
          7803
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.colSampleByTree
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.colSampleByTree
        </td>
      </tr><tr>
        <td>
          153
        </td>
        <td>
          1188
        </td>
        <td>
          7974
          -
          7993
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;colsample_bylevel&quot;
        </td>
      </tr><tr>
        <td>
          153
        </td>
        <td>
          1191
        </td>
        <td>
          7952
          -
          8160
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;colsample_bylevel&quot;, &quot;Subsample ratio of columns for each split, in each level. Must be in (0, 1].&quot;, org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0, false, true))
        </td>
      </tr><tr>
        <td>
          154
        </td>
        <td>
          1189
        </td>
        <td>
          7999
          -
          8077
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Subsample ratio of columns for each split, in each level. Must be in (0, 1].&quot;
        </td>
      </tr><tr>
        <td>
          155
        </td>
        <td>
          1190
        </td>
        <td>
          8083
          -
          8159
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inRange
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0, false, true)
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          1192
        </td>
        <td>
          8199
          -
          8215
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.colSampleByLevel
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.colSampleByLevel
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          1193
        </td>
        <td>
          8197
          -
          8216
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.colSampleByLevel)
        </td>
      </tr><tr>
        <td>
          161
        </td>
        <td>
          1196
        </td>
        <td>
          8367
          -
          8495
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;lambda&quot;, &quot;L2 regularization term on weights, increase this value will make model more conservative.&quot;)
        </td>
      </tr><tr>
        <td>
          161
        </td>
        <td>
          1194
        </td>
        <td>
          8389
          -
          8397
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;lambda&quot;
        </td>
      </tr><tr>
        <td>
          162
        </td>
        <td>
          1195
        </td>
        <td>
          8403
          -
          8494
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;L2 regularization term on weights, increase this value will make model more conservative.&quot;
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          1198
        </td>
        <td>
          8522
          -
          8531
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.lambda)
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          1197
        </td>
        <td>
          8524
          -
          8530
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.lambda
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.lambda
        </td>
      </tr><tr>
        <td>
          168
        </td>
        <td>
          1201
        </td>
        <td>
          8681
          -
          8808
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;alpha&quot;, &quot;L1 regularization term on weights, increase this value will make model more conservative.&quot;)
        </td>
      </tr><tr>
        <td>
          168
        </td>
        <td>
          1199
        </td>
        <td>
          8703
          -
          8710
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;alpha&quot;
        </td>
      </tr><tr>
        <td>
          169
        </td>
        <td>
          1200
        </td>
        <td>
          8716
          -
          8807
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;L1 regularization term on weights, increase this value will make model more conservative.&quot;
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          1203
        </td>
        <td>
          8834
          -
          8842
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.alpha)
        </td>
      </tr><tr>
        <td>
          170
        </td>
        <td>
          1202
        </td>
        <td>
          8836
          -
          8841
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.alpha
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.alpha
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          1204
        </td>
        <td>
          9013
          -
          9026
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;tree_method&quot;
        </td>
      </tr><tr>
        <td>
          175
        </td>
        <td>
          1212
        </td>
        <td>
          8997
          -
          9188
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Param.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.Param[String](this, &quot;tree_method&quot;, &quot;The tree construction algorithm used in XGBoost. Can be auto, exact, approx, hist.&quot;, org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;auto&quot;, &quot;exact&quot;, &quot;approx&quot;, &quot;hist&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))))
        </td>
      </tr><tr>
        <td>
          176
        </td>
        <td>
          1205
        </td>
        <td>
          9032
          -
          9116
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;The tree construction algorithm used in XGBoost. Can be auto, exact, approx, hist.&quot;
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          1207
        </td>
        <td>
          9160
          -
          9167
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;exact&quot;
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          1210
        </td>
        <td>
          9146
          -
          9186
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;auto&quot;, &quot;exact&quot;, &quot;approx&quot;, &quot;hist&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          1209
        </td>
        <td>
          9179
          -
          9185
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;hist&quot;
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          1206
        </td>
        <td>
          9152
          -
          9158
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;auto&quot;
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          1208
        </td>
        <td>
          9169
          -
          9177
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;approx&quot;
        </td>
      </tr><tr>
        <td>
          177
        </td>
        <td>
          1211
        </td>
        <td>
          9122
          -
          9187
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;auto&quot;, &quot;exact&quot;, &quot;approx&quot;, &quot;hist&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          1214
        </td>
        <td>
          9219
          -
          9232
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[String](XGBoostParams.this.treeMethod)
        </td>
      </tr><tr>
        <td>
          178
        </td>
        <td>
          1213
        </td>
        <td>
          9221
          -
          9231
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.treeMethod
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.treeMethod
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          1218
        </td>
        <td>
          9528
          -
          9889
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;sketch_eps&quot;, &quot;Used only for approximate greedy algorithm. Translates into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. Must be in (0, 1). &quot;, org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0, false, false))
        </td>
      </tr><tr>
        <td>
          185
        </td>
        <td>
          1215
        </td>
        <td>
          9550
          -
          9562
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;sketch_eps&quot;
        </td>
      </tr><tr>
        <td>
          187
        </td>
        <td>
          1216
        </td>
        <td>
          9568
          -
          9805
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Used only for approximate greedy algorithm. Translates into O(1 / sketch_eps) number of bins. Compared to directly select number of bins, this comes with theoretical guarantee with sketch accuracy. Must be in (0, 1). &quot;
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          1217
        </td>
        <td>
          9811
          -
          9888
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inRange
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0, false, false)
        </td>
      </tr><tr>
        <td>
          190
        </td>
        <td>
          1219
        </td>
        <td>
          9921
          -
          9930
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.sketchEps
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.sketchEps
        </td>
      </tr><tr>
        <td>
          190
        </td>
        <td>
          1220
        </td>
        <td>
          9919
          -
          9931
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.sketchEps)
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          1221
        </td>
        <td>
          10191
          -
          10209
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;scale_pos_weight&quot;
        </td>
      </tr><tr>
        <td>
          196
        </td>
        <td>
          1223
        </td>
        <td>
          10169
          -
          10301
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;scale_pos_weight&quot;, &quot;Scale the weight of positive examples by this factor. Useful for unbalanced classes&quot;)
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          1222
        </td>
        <td>
          10215
          -
          10300
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Scale the weight of positive examples by this factor. Useful for unbalanced classes&quot;
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          1225
        </td>
        <td>
          10336
          -
          10353
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.scalePosWeight)
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          1224
        </td>
        <td>
          10338
          -
          10352
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.scalePosWeight
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.scalePosWeight
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          1226
        </td>
        <td>
          10601
          -
          10610
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;updater&quot;
        </td>
      </tr><tr>
        <td>
          204
        </td>
        <td>
          1229
        </td>
        <td>
          10585
          -
          10888
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Param.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.Param[String](this, &quot;updater&quot;, &quot;A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. This is an advanced parameter that is usually set automatically, depending on some other parameters&quot;, XGBoostParams.this.updaterValidator)
        </td>
      </tr><tr>
        <td>
          206
        </td>
        <td>
          1227
        </td>
        <td>
          10616
          -
          10869
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees. This is an advanced parameter that is usually set automatically, depending on some other parameters&quot;
        </td>
      </tr><tr>
        <td>
          207
        </td>
        <td>
          1228
        </td>
        <td>
          10871
          -
          10887
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.updaterValidator
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.updaterValidator
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          1231
        </td>
        <td>
          10916
          -
          10926
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[String](XGBoostParams.this.updater)
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          1230
        </td>
        <td>
          10918
          -
          10925
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.updater
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.updater
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1237
        </td>
        <td>
          11005
          -
          11064
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.map
        </td>
        <td style="background: #AEF1AE">
          scala.this.Predef.refArrayOps[String](value.split(&quot;,&quot;)).map[Boolean, Array[Boolean]](((ud: String) =&gt; scala.this.Predef.refArrayOps[String](XGBoostParams.this.updaterValues).contains[String](ud.trim())))(scala.this.Array.canBuildFrom[Boolean]((ClassTag.Boolean: scala.reflect.ClassTag[Boolean])))
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1234
        </td>
        <td>
          11055
          -
          11062
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.trim
        </td>
        <td style="background: #AEF1AE">
          ud.trim()
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1236
        </td>
        <td>
          11025
          -
          11025
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          scala.this.Array.canBuildFrom[Boolean]((ClassTag.Boolean: scala.reflect.ClassTag[Boolean]))
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1239
        </td>
        <td>
          11005
          -
          11079
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.TraversableOnce.reduce
        </td>
        <td style="background: #AEF1AE">
          scala.this.Predef.booleanArrayOps(scala.this.Predef.refArrayOps[String](value.split(&quot;,&quot;)).map[Boolean, Array[Boolean]](((ud: String) =&gt; scala.this.Predef.refArrayOps[String](XGBoostParams.this.updaterValues).contains[String](ud.trim())))(scala.this.Array.canBuildFrom[Boolean]((ClassTag.Boolean: scala.reflect.ClassTag[Boolean])))).reduce[Boolean](((x$1: Boolean, x$2: Boolean) =&gt; x$1.&amp;&amp;(x$2)))
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1233
        </td>
        <td>
          11032
          -
          11045
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.updaterValues
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.updaterValues
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1232
        </td>
        <td>
          11005
          -
          11021
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.String.split
        </td>
        <td style="background: #AEF1AE">
          value.split(&quot;,&quot;)
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1235
        </td>
        <td>
          11032
          -
          11063
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.SeqLike.contains
        </td>
        <td style="background: #AEF1AE">
          scala.this.Predef.refArrayOps[String](XGBoostParams.this.updaterValues).contains[String](ud.trim())
        </td>
      </tr><tr>
        <td>
          211
        </td>
        <td>
          1238
        </td>
        <td>
          11072
          -
          11078
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          x$1.&amp;&amp;(x$2)
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          1240
        </td>
        <td>
          11121
          -
          11136
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;grow_colmaker&quot;
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          1248
        </td>
        <td>
          11115
          -
          11238
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;grow_colmaker&quot;, &quot;distcol&quot;, &quot;grow_histmaker&quot;, &quot;grow_local_histmaker&quot;, &quot;grow_skmaker&quot;, &quot;sync&quot;, &quot;refresh&quot;, &quot;prune&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          1242
        </td>
        <td>
          11149
          -
          11165
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;grow_histmaker&quot;
        </td>
      </tr><tr>
        <td>
          214
        </td>
        <td>
          1241
        </td>
        <td>
          11138
          -
          11147
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;distcol&quot;
        </td>
      </tr><tr>
        <td>
          215
        </td>
        <td>
          1246
        </td>
        <td>
          11219
          -
          11228
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;refresh&quot;
        </td>
      </tr><tr>
        <td>
          215
        </td>
        <td>
          1243
        </td>
        <td>
          11171
          -
          11193
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;grow_local_histmaker&quot;
        </td>
      </tr><tr>
        <td>
          215
        </td>
        <td>
          1245
        </td>
        <td>
          11211
          -
          11217
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;sync&quot;
        </td>
      </tr><tr>
        <td>
          215
        </td>
        <td>
          1244
        </td>
        <td>
          11195
          -
          11209
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;grow_skmaker&quot;
        </td>
      </tr><tr>
        <td>
          215
        </td>
        <td>
          1247
        </td>
        <td>
          11230
          -
          11237
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;prune&quot;
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          1249
        </td>
        <td>
          11487
          -
          11501
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;refresh_leaf&quot;
        </td>
      </tr><tr>
        <td>
          221
        </td>
        <td>
          1253
        </td>
        <td>
          11468
          -
          11729
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.IntParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.IntParam(this, &quot;refresh_leaf&quot;, &quot;A parameter of the \'refresh\' updater plugin. When this flag is true, tree leafs as well as tree nodes\' stats are updated. When it is false, only node stats are updated.&quot;, org.apache.spark.ml.param.ParamValidators.inArray[Int](scala.Array.apply(0, 1)))
        </td>
      </tr><tr>
        <td>
          222
        </td>
        <td>
          1250
        </td>
        <td>
          11507
          -
          11686
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;A parameter of the \'refresh\' updater plugin. When this flag is true, tree leafs as well as tree nodes\' stats are updated. When it is false, only node stats are updated.&quot;
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          1252
        </td>
        <td>
          11692
          -
          11728
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[Int](scala.Array.apply(0, 1))
        </td>
      </tr><tr>
        <td>
          224
        </td>
        <td>
          1251
        </td>
        <td>
          11716
          -
          11727
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply(0, 1)
        </td>
      </tr><tr>
        <td>
          225
        </td>
        <td>
          1255
        </td>
        <td>
          11758
          -
          11772
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Int](XGBoostParams.this.refreshLeaf)
        </td>
      </tr><tr>
        <td>
          225
        </td>
        <td>
          1254
        </td>
        <td>
          11760
          -
          11771
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.refreshLeaf
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.refreshLeaf
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          1262
        </td>
        <td>
          11911
          -
          12069
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Param.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.Param[String](this, &quot;process_type&quot;, &quot;The type of boosting process to run. Can be default or update.&quot;, org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;default&quot;, &quot;update&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))))
        </td>
      </tr><tr>
        <td>
          230
        </td>
        <td>
          1256
        </td>
        <td>
          11927
          -
          11941
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;process_type&quot;
        </td>
      </tr><tr>
        <td>
          231
        </td>
        <td>
          1257
        </td>
        <td>
          11947
          -
          12011
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;The type of boosting process to run. Can be default or update.&quot;
        </td>
      </tr><tr>
        <td>
          232
        </td>
        <td>
          1258
        </td>
        <td>
          12047
          -
          12056
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;default&quot;
        </td>
      </tr><tr>
        <td>
          232
        </td>
        <td>
          1261
        </td>
        <td>
          12017
          -
          12068
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;default&quot;, &quot;update&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          232
        </td>
        <td>
          1260
        </td>
        <td>
          12041
          -
          12067
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;default&quot;, &quot;update&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          232
        </td>
        <td>
          1259
        </td>
        <td>
          12058
          -
          12066
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;update&quot;
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          1264
        </td>
        <td>
          12101
          -
          12115
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[String](XGBoostParams.this.processType)
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          1263
        </td>
        <td>
          12103
          -
          12114
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.processType
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.processType
        </td>
      </tr><tr>
        <td>
          239
        </td>
        <td>
          1271
        </td>
        <td>
          12344
          -
          12528
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Param.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.Param[String](this, &quot;grow_policy&quot;, &quot;Controls the way new nodes are added to the tree. Can be \'depthwise\' or \'lossguide\'.&quot;, org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;depthwise&quot;, &quot;lossguide&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))))
        </td>
      </tr><tr>
        <td>
          239
        </td>
        <td>
          1265
        </td>
        <td>
          12360
          -
          12373
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;grow_policy&quot;
        </td>
      </tr><tr>
        <td>
          240
        </td>
        <td>
          1266
        </td>
        <td>
          12379
          -
          12465
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Controls the way new nodes are added to the tree. Can be \'depthwise\' or \'lossguide\'.&quot;
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          1267
        </td>
        <td>
          12501
          -
          12512
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;depthwise&quot;
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          1270
        </td>
        <td>
          12471
          -
          12527
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;depthwise&quot;, &quot;lossguide&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          1269
        </td>
        <td>
          12495
          -
          12526
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;depthwise&quot;, &quot;lossguide&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          241
        </td>
        <td>
          1268
        </td>
        <td>
          12514
          -
          12525
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;lossguide&quot;
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          1273
        </td>
        <td>
          12559
          -
          12572
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[String](XGBoostParams.this.growPolicy)
        </td>
      </tr><tr>
        <td>
          242
        </td>
        <td>
          1272
        </td>
        <td>
          12561
          -
          12571
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.growPolicy
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.growPolicy
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          1277
        </td>
        <td>
          12726
          -
          12904
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.IntParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.IntParam(this, &quot;max_leaves&quot;, &quot;Maximum number of nodes to be added. Only relevant for the \'lossguide\' grow policy. Must be &gt;= 0. &quot;, org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0))
        </td>
      </tr><tr>
        <td>
          247
        </td>
        <td>
          1274
        </td>
        <td>
          12745
          -
          12757
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;max_leaves&quot;
        </td>
      </tr><tr>
        <td>
          248
        </td>
        <td>
          1275
        </td>
        <td>
          12763
          -
          12874
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Maximum number of nodes to be added. Only relevant for the \'lossguide\' grow policy. Must be &gt;= 0. &quot;
        </td>
      </tr><tr>
        <td>
          250
        </td>
        <td>
          1276
        </td>
        <td>
          12880
          -
          12903
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.gtEq
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.gtEq[Any](0.0)
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          1279
        </td>
        <td>
          12931
          -
          12943
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Int](XGBoostParams.this.maxLeaves)
        </td>
      </tr><tr>
        <td>
          251
        </td>
        <td>
          1278
        </td>
        <td>
          12933
          -
          12942
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.maxLeaves
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.maxLeaves
        </td>
      </tr><tr>
        <td>
          256
        </td>
        <td>
          1283
        </td>
        <td>
          13097
          -
          13288
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.IntParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.IntParam(this, &quot;max_bin&quot;, &quot;Maximum number of discrete bins to bucket continuous features. This is only used if \'hist\' is specified as tree_method. &quot;, org.apache.spark.ml.param.ParamValidators.gtEq[Any](1.0))
        </td>
      </tr><tr>
        <td>
          256
        </td>
        <td>
          1280
        </td>
        <td>
          13116
          -
          13125
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;max_bin&quot;
        </td>
      </tr><tr>
        <td>
          257
        </td>
        <td>
          1281
        </td>
        <td>
          13131
          -
          13262
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Maximum number of discrete bins to bucket continuous features. This is only used if \'hist\' is specified as tree_method. &quot;
        </td>
      </tr><tr>
        <td>
          258
        </td>
        <td>
          1282
        </td>
        <td>
          13264
          -
          13287
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.gtEq
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.gtEq[Any](1.0)
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          1285
        </td>
        <td>
          13312
          -
          13321
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Int](XGBoostParams.this.maxBin)
        </td>
      </tr><tr>
        <td>
          259
        </td>
        <td>
          1284
        </td>
        <td>
          13314
          -
          13320
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.maxBin
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.maxBin
        </td>
      </tr><tr>
        <td>
          265
        </td>
        <td>
          1292
        </td>
        <td>
          13488
          -
          13778
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Param.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.Param[String](this, &quot;sample_type&quot;, &quot;Type of sampling algorithm. Can be \'uniform\' or \'weighted\'. \&quot;uniform\&quot;: dropped trees are selected uniformly.\&quot;weighted\&quot;: dropped trees are selected in proportion to weight.&quot;, org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;uniform&quot;, &quot;weighted&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))))
        </td>
      </tr><tr>
        <td>
          265
        </td>
        <td>
          1286
        </td>
        <td>
          13504
          -
          13517
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;sample_type&quot;
        </td>
      </tr><tr>
        <td>
          267
        </td>
        <td>
          1287
        </td>
        <td>
          13523
          -
          13718
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Type of sampling algorithm. Can be \'uniform\' or \'weighted\'. \&quot;uniform\&quot;: dropped trees are selected uniformly.\&quot;weighted\&quot;: dropped trees are selected in proportion to weight.&quot;
        </td>
      </tr><tr>
        <td>
          269
        </td>
        <td>
          1291
        </td>
        <td>
          13724
          -
          13777
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;uniform&quot;, &quot;weighted&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          269
        </td>
        <td>
          1288
        </td>
        <td>
          13754
          -
          13763
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;uniform&quot;
        </td>
      </tr><tr>
        <td>
          269
        </td>
        <td>
          1290
        </td>
        <td>
          13748
          -
          13776
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;uniform&quot;, &quot;weighted&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          269
        </td>
        <td>
          1289
        </td>
        <td>
          13765
          -
          13775
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;weighted&quot;
        </td>
      </tr><tr>
        <td>
          270
        </td>
        <td>
          1294
        </td>
        <td>
          13809
          -
          13822
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[String](XGBoostParams.this.sampleType)
        </td>
      </tr><tr>
        <td>
          270
        </td>
        <td>
          1293
        </td>
        <td>
          13811
          -
          13821
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.sampleType
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.sampleType
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1301
        </td>
        <td>
          13957
          -
          14271
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Param.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.Param[String](this, &quot;normalize_type&quot;, &quot;Type of normalization algorithm. Can be \'tree\' or \'forest\'.\&quot;tree\&quot;: new trees have the same weight of each of dropped trees.\&quot;forest\&quot;: new trees have the same weight of sum of dropped trees (forest).&quot;, org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;tree&quot;, &quot;forest&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))))
        </td>
      </tr><tr>
        <td>
          275
        </td>
        <td>
          1295
        </td>
        <td>
          13973
          -
          13989
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;normalize_type&quot;
        </td>
      </tr><tr>
        <td>
          277
        </td>
        <td>
          1296
        </td>
        <td>
          13995
          -
          14216
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Type of normalization algorithm. Can be \'tree\' or \'forest\'.\&quot;tree\&quot;: new trees have the same weight of each of dropped trees.\&quot;forest\&quot;: new trees have the same weight of sum of dropped trees (forest).&quot;
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          1297
        </td>
        <td>
          14252
          -
          14258
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;tree&quot;
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          1300
        </td>
        <td>
          14222
          -
          14270
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;tree&quot;, &quot;forest&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          1299
        </td>
        <td>
          14246
          -
          14269
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;tree&quot;, &quot;forest&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          279
        </td>
        <td>
          1298
        </td>
        <td>
          14260
          -
          14268
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;forest&quot;
        </td>
      </tr><tr>
        <td>
          280
        </td>
        <td>
          1303
        </td>
        <td>
          14305
          -
          14321
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[String](XGBoostParams.this.normalizeType)
        </td>
      </tr><tr>
        <td>
          280
        </td>
        <td>
          1302
        </td>
        <td>
          14307
          -
          14320
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.normalizeType
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.normalizeType
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          1304
        </td>
        <td>
          14499
          -
          14510
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;rate_drop&quot;
        </td>
      </tr><tr>
        <td>
          285
        </td>
        <td>
          1307
        </td>
        <td>
          14477
          -
          14649
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;rate_drop&quot;, &quot;dropout rate (a fraction of previous trees to drop during the dropout). Must be in [0, 1]. &quot;, org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0))
        </td>
      </tr><tr>
        <td>
          286
        </td>
        <td>
          1305
        </td>
        <td>
          14516
          -
          14609
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;dropout rate (a fraction of previous trees to drop during the dropout). Must be in [0, 1]. &quot;
        </td>
      </tr><tr>
        <td>
          287
        </td>
        <td>
          1306
        </td>
        <td>
          14615
          -
          14648
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inRange
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0)
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          1309
        </td>
        <td>
          14678
          -
          14689
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.rateDrop)
        </td>
      </tr><tr>
        <td>
          288
        </td>
        <td>
          1308
        </td>
        <td>
          14680
          -
          14688
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.rateDrop
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.rateDrop
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          1314
        </td>
        <td>
          14802
          -
          14936
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.IntParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.IntParam(this, &quot;one_drop&quot;, &quot;whether to drop at least one tree during the dropout. &quot;, org.apache.spark.ml.param.ParamValidators.inArray[Int](scala.Array.apply(0, 1)))
        </td>
      </tr><tr>
        <td>
          293
        </td>
        <td>
          1310
        </td>
        <td>
          14821
          -
          14831
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;one_drop&quot;
        </td>
      </tr><tr>
        <td>
          294
        </td>
        <td>
          1311
        </td>
        <td>
          14837
          -
          14893
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;whether to drop at least one tree during the dropout. &quot;
        </td>
      </tr><tr>
        <td>
          295
        </td>
        <td>
          1312
        </td>
        <td>
          14923
          -
          14934
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply(0, 1)
        </td>
      </tr><tr>
        <td>
          295
        </td>
        <td>
          1313
        </td>
        <td>
          14899
          -
          14935
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[Int](scala.Array.apply(0, 1))
        </td>
      </tr><tr>
        <td>
          296
        </td>
        <td>
          1315
        </td>
        <td>
          14963
          -
          14970
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.oneDrop
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.oneDrop
        </td>
      </tr><tr>
        <td>
          296
        </td>
        <td>
          1316
        </td>
        <td>
          14961
          -
          14971
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Int](XGBoostParams.this.oneDrop)
        </td>
      </tr><tr>
        <td>
          301
        </td>
        <td>
          1320
        </td>
        <td>
          15127
          -
          15301
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;skip_drop&quot;, &quot;Probability of skipping the dropout procedure during a boosting iteration. Must be in [0, 1].&quot;, org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0))
        </td>
      </tr><tr>
        <td>
          301
        </td>
        <td>
          1317
        </td>
        <td>
          15149
          -
          15160
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;skip_drop&quot;
        </td>
      </tr><tr>
        <td>
          302
        </td>
        <td>
          1318
        </td>
        <td>
          15166
          -
          15261
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Probability of skipping the dropout procedure during a boosting iteration. Must be in [0, 1].&quot;
        </td>
      </tr><tr>
        <td>
          303
        </td>
        <td>
          1319
        </td>
        <td>
          15267
          -
          15300
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inRange
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0)
        </td>
      </tr><tr>
        <td>
          304
        </td>
        <td>
          1321
        </td>
        <td>
          15332
          -
          15340
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.skipDrop
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.skipDrop
        </td>
      </tr><tr>
        <td>
          304
        </td>
        <td>
          1322
        </td>
        <td>
          15330
          -
          15341
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.skipDrop)
        </td>
      </tr><tr>
        <td>
          310
        </td>
        <td>
          1323
        </td>
        <td>
          15519
          -
          15532
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;lambda_bias&quot;
        </td>
      </tr><tr>
        <td>
          310
        </td>
        <td>
          1326
        </td>
        <td>
          15497
          -
          15622
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;lambda_bias&quot;, &quot;L2 regularization term on bias. Must be in [0, 1].&quot;, org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0))
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          1324
        </td>
        <td>
          15538
          -
          15590
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;L2 regularization term on bias. Must be in [0, 1].&quot;
        </td>
      </tr><tr>
        <td>
          311
        </td>
        <td>
          1325
        </td>
        <td>
          15592
          -
          15621
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inRange
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inRange[Any](0.0, 1.0)
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          1327
        </td>
        <td>
          15655
          -
          15665
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.lambdaBias
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.lambdaBias
        </td>
      </tr><tr>
        <td>
          312
        </td>
        <td>
          1328
        </td>
        <td>
          15653
          -
          15666
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.lambdaBias)
        </td>
      </tr><tr>
        <td>
          319
        </td>
        <td>
          1329
        </td>
        <td>
          15893
          -
          15917
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;tweedie_variance_power&quot;
        </td>
      </tr><tr>
        <td>
          319
        </td>
        <td>
          1332
        </td>
        <td>
          15871
          -
          16093
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;tweedie_variance_power&quot;, &quot;parameter that controls the variance of the Tweedie distribution. Must be in (1, 2).&quot;, org.apache.spark.ml.param.ParamValidators.inRange[Any](1.0, 2.0, false, false))
        </td>
      </tr><tr>
        <td>
          320
        </td>
        <td>
          1330
        </td>
        <td>
          15923
          -
          16009
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;parameter that controls the variance of the Tweedie distribution. Must be in (1, 2).&quot;
        </td>
      </tr><tr>
        <td>
          321
        </td>
        <td>
          1331
        </td>
        <td>
          16015
          -
          16092
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inRange
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inRange[Any](1.0, 2.0, false, false)
        </td>
      </tr><tr>
        <td>
          322
        </td>
        <td>
          1333
        </td>
        <td>
          16136
          -
          16156
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.tweedieVariancePower
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.tweedieVariancePower
        </td>
      </tr><tr>
        <td>
          322
        </td>
        <td>
          1334
        </td>
        <td>
          16134
          -
          16157
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.tweedieVariancePower)
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          1335
        </td>
        <td>
          16352
          -
          16363
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;objective&quot;
        </td>
      </tr><tr>
        <td>
          328
        </td>
        <td>
          1349
        </td>
        <td>
          16336
          -
          18052
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Param.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.Param[String](this, &quot;objective&quot;, &quot;Specifies the learning objective.\&quot;reg:linear\&quot; -- linear regression \&quot;reg:logistic\&quot; --logistic regression \&quot;binary:logistic\&quot; --logistic regression for binary classification, output is probability \&quot;binary:logitraw\&quot; --logistic regression for binary classification, output is score before logistic transformation \&quot;count:poisson\&quot; --poisson regression for count data, output mean of poisson distribution max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) \&quot;multi:softmax\&quot; --multiclass classification using the softmax objective. You also need to set num_class(number of classes)\&quot;multi:softprob\&quot; --same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata, nclass matrix. The result contains predicted probability of each data point belonging to each class. \&quot;rank:pairwise\&quot; --set XGBoost to do ranking task by minimizing the pairwise loss \&quot;reg:gamma\&quot; --gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed\&quot;reg:tweedie\&quot; --Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed.&quot;, org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;reg:linear&quot;, &quot;reg:logistic&quot;, &quot;binary:logistic&quot;, &quot;binary:logistraw&quot;, &quot;count:poisson&quot;, &quot;multi:softmax&quot;, &quot;multi:softprob&quot;, &quot;rank:pairwise&quot;, &quot;reg:gamma&quot;, &quot;reg:tweedie&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))))
        </td>
      </tr><tr>
        <td>
          346
        </td>
        <td>
          1336
        </td>
        <td>
          16365
          -
          17837
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Specifies the learning objective.\&quot;reg:linear\&quot; -- linear regression \&quot;reg:logistic\&quot; --logistic regression \&quot;binary:logistic\&quot; --logistic regression for binary classification, output is probability \&quot;binary:logitraw\&quot; --logistic regression for binary classification, output is score before logistic transformation \&quot;count:poisson\&quot; --poisson regression for count data, output mean of poisson distribution max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization) \&quot;multi:softmax\&quot; --multiclass classification using the softmax objective. You also need to set num_class(number of classes)\&quot;multi:softprob\&quot; --same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata, nclass matrix. The result contains predicted probability of each data point belonging to each class. \&quot;rank:pairwise\&quot; --set XGBoost to do ranking task by minimizing the pairwise loss \&quot;reg:gamma\&quot; --gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed\&quot;reg:tweedie\&quot; --Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed.&quot;
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          1339
        </td>
        <td>
          17903
          -
          17920
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;binary:logistic&quot;
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          1348
        </td>
        <td>
          17843
          -
          18051
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;reg:linear&quot;, &quot;reg:logistic&quot;, &quot;binary:logistic&quot;, &quot;binary:logistraw&quot;, &quot;count:poisson&quot;, &quot;multi:softmax&quot;, &quot;multi:softprob&quot;, &quot;rank:pairwise&quot;, &quot;reg:gamma&quot;, &quot;reg:tweedie&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          1338
        </td>
        <td>
          17887
          -
          17901
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;reg:logistic&quot;
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          1347
        </td>
        <td>
          17867
          -
          18050
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;reg:linear&quot;, &quot;reg:logistic&quot;, &quot;binary:logistic&quot;, &quot;binary:logistraw&quot;, &quot;count:poisson&quot;, &quot;multi:softmax&quot;, &quot;multi:softprob&quot;, &quot;rank:pairwise&quot;, &quot;reg:gamma&quot;, &quot;reg:tweedie&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          348
        </td>
        <td>
          1337
        </td>
        <td>
          17873
          -
          17885
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;reg:linear&quot;
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          1342
        </td>
        <td>
          17965
          -
          17980
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;multi:softmax&quot;
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          1341
        </td>
        <td>
          17948
          -
          17963
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;count:poisson&quot;
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          1343
        </td>
        <td>
          17982
          -
          17998
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;multi:softprob&quot;
        </td>
      </tr><tr>
        <td>
          349
        </td>
        <td>
          1340
        </td>
        <td>
          17928
          -
          17946
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;binary:logistraw&quot;
        </td>
      </tr><tr>
        <td>
          350
        </td>
        <td>
          1345
        </td>
        <td>
          18023
          -
          18034
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;reg:gamma&quot;
        </td>
      </tr><tr>
        <td>
          350
        </td>
        <td>
          1344
        </td>
        <td>
          18006
          -
          18021
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;rank:pairwise&quot;
        </td>
      </tr><tr>
        <td>
          350
        </td>
        <td>
          1346
        </td>
        <td>
          18036
          -
          18049
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;reg:tweedie&quot;
        </td>
      </tr><tr>
        <td>
          351
        </td>
        <td>
          1351
        </td>
        <td>
          18082
          -
          18094
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[String](XGBoostParams.this.objective)
        </td>
      </tr><tr>
        <td>
          351
        </td>
        <td>
          1350
        </td>
        <td>
          18084
          -
          18093
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.objective
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.objective
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          1352
        </td>
        <td>
          18219
          -
          18230
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;num_class&quot;
        </td>
      </tr><tr>
        <td>
          356
        </td>
        <td>
          1355
        </td>
        <td>
          18200
          -
          18281
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.IntParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.IntParam(this, &quot;num_class&quot;, &quot;Number of classes&quot;, org.apache.spark.ml.param.ParamValidators.gtEq[Any](1.0))
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          1354
        </td>
        <td>
          18257
          -
          18280
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.gtEq
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.gtEq[Any](1.0)
        </td>
      </tr><tr>
        <td>
          357
        </td>
        <td>
          1353
        </td>
        <td>
          18236
          -
          18255
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Number of classes&quot;
        </td>
      </tr><tr>
        <td>
          358
        </td>
        <td>
          1357
        </td>
        <td>
          18309
          -
          18322
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Int](XGBoostParams.this.numClasses)
        </td>
      </tr><tr>
        <td>
          358
        </td>
        <td>
          1356
        </td>
        <td>
          18311
          -
          18321
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.numClasses
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.numClasses
        </td>
      </tr><tr>
        <td>
          363
        </td>
        <td>
          1360
        </td>
        <td>
          18448
          -
          18549
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;base_score&quot;, &quot;the initial prediction score of all instances, global bias&quot;)
        </td>
      </tr><tr>
        <td>
          363
        </td>
        <td>
          1358
        </td>
        <td>
          18470
          -
          18482
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;base_score&quot;
        </td>
      </tr><tr>
        <td>
          364
        </td>
        <td>
          1359
        </td>
        <td>
          18488
          -
          18548
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;the initial prediction score of all instances, global bias&quot;
        </td>
      </tr><tr>
        <td>
          365
        </td>
        <td>
          1362
        </td>
        <td>
          18579
          -
          18591
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.baseScore)
        </td>
      </tr><tr>
        <td>
          365
        </td>
        <td>
          1361
        </td>
        <td>
          18581
          -
          18590
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.baseScore
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.baseScore
        </td>
      </tr><tr>
        <td>
          371
        </td>
        <td>
          1363
        </td>
        <td>
          18870
          -
          18883
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;eval_metric&quot;
        </td>
      </tr><tr>
        <td>
          371
        </td>
        <td>
          1382
        </td>
        <td>
          18854
          -
          19272
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Param.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.Param[String](this, &quot;eval_metric&quot;, &quot;Evaluation metrics for validation data. A default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking)&quot;, org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;rmse&quot;, &quot;mae&quot;, &quot;logloss&quot;, &quot;error&quot;, &quot;error@t&quot;, &quot;merror&quot;, &quot;mlogloss&quot;, &quot;auc&quot;, &quot;ndcg&quot;, &quot;map&quot;, &quot;ndcg@n&quot;, &quot;ndcg-&quot;, &quot;ndcg@n-&quot;, &quot;map-&quot;, &quot;map@n-&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))))
        </td>
      </tr><tr>
        <td>
          373
        </td>
        <td>
          1364
        </td>
        <td>
          18889
          -
          19093
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Evaluation metrics for validation data. A default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking)&quot;
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          1366
        </td>
        <td>
          19137
          -
          19142
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;mae&quot;
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          1369
        </td>
        <td>
          19164
          -
          19173
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;error@t&quot;
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          1381
        </td>
        <td>
          19099
          -
          19271
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.inArray
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.inArray[String](scala.Array.apply[String](&quot;rmse&quot;, &quot;mae&quot;, &quot;logloss&quot;, &quot;error&quot;, &quot;error@t&quot;, &quot;merror&quot;, &quot;mlogloss&quot;, &quot;auc&quot;, &quot;ndcg&quot;, &quot;map&quot;, &quot;ndcg@n&quot;, &quot;ndcg-&quot;, &quot;ndcg@n-&quot;, &quot;map-&quot;, &quot;map@n-&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String])))
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          1380
        </td>
        <td>
          19123
          -
          19270
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.Array.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Array.apply[String](&quot;rmse&quot;, &quot;mae&quot;, &quot;logloss&quot;, &quot;error&quot;, &quot;error@t&quot;, &quot;merror&quot;, &quot;mlogloss&quot;, &quot;auc&quot;, &quot;ndcg&quot;, &quot;map&quot;, &quot;ndcg@n&quot;, &quot;ndcg-&quot;, &quot;ndcg@n-&quot;, &quot;map-&quot;, &quot;map@n-&quot;)((ClassTag.apply[String](classOf[java.lang.String]): scala.reflect.ClassTag[String]))
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          1365
        </td>
        <td>
          19129
          -
          19135
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;rmse&quot;
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          1368
        </td>
        <td>
          19155
          -
          19162
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;error&quot;
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          1370
        </td>
        <td>
          19175
          -
          19183
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;merror&quot;
        </td>
      </tr><tr>
        <td>
          375
        </td>
        <td>
          1367
        </td>
        <td>
          19144
          -
          19153
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;logloss&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          1375
        </td>
        <td>
          19223
          -
          19231
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;ndcg@n&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          1378
        </td>
        <td>
          19253
          -
          19259
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;map-&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          1372
        </td>
        <td>
          19201
          -
          19206
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;auc&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          1374
        </td>
        <td>
          19216
          -
          19221
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;map&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          1377
        </td>
        <td>
          19242
          -
          19251
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;ndcg@n-&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          1371
        </td>
        <td>
          19189
          -
          19199
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;mlogloss&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          1379
        </td>
        <td>
          19261
          -
          19269
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;map@n-&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          1373
        </td>
        <td>
          19208
          -
          19214
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;ndcg&quot;
        </td>
      </tr><tr>
        <td>
          376
        </td>
        <td>
          1376
        </td>
        <td>
          19233
          -
          19240
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;ndcg-&quot;
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          1384
        </td>
        <td>
          19303
          -
          19316
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[String](XGBoostParams.this.evalMetric)
        </td>
      </tr><tr>
        <td>
          377
        </td>
        <td>
          1383
        </td>
        <td>
          19305
          -
          19315
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.evalMetric
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.evalMetric
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          1387
        </td>
        <td>
          19395
          -
          19447
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.DoubleParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.DoubleParam(this, &quot;seed&quot;, &quot;Random number seed.&quot;)
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          1386
        </td>
        <td>
          19425
          -
          19446
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Random number seed.&quot;
        </td>
      </tr><tr>
        <td>
          382
        </td>
        <td>
          1385
        </td>
        <td>
          19417
          -
          19423
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;seed&quot;
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          1389
        </td>
        <td>
          19472
          -
          19479
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Double](XGBoostParams.this.seed)
        </td>
      </tr><tr>
        <td>
          383
        </td>
        <td>
          1388
        </td>
        <td>
          19474
          -
          19478
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.seed
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.seed
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          1393
        </td>
        <td>
          19591
          -
          19687
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.IntParam.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.ml.param.IntParam(this, &quot;num_round&quot;, &quot;Number of rounds. Must be &gt;= 1. &quot;, org.apache.spark.ml.param.ParamValidators.gtEq[Any](1.0))
        </td>
      </tr><tr>
        <td>
          388
        </td>
        <td>
          1390
        </td>
        <td>
          19610
          -
          19621
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;num_round&quot;
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          1392
        </td>
        <td>
          19663
          -
          19686
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.ParamValidators.gtEq
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.ml.param.ParamValidators.gtEq[Any](1.0)
        </td>
      </tr><tr>
        <td>
          389
        </td>
        <td>
          1391
        </td>
        <td>
          19627
          -
          19661
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;Number of rounds. Must be &gt;= 1. &quot;
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          1395
        </td>
        <td>
          19713
          -
          19724
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.$[Int](XGBoostParams.this.numRound)
        </td>
      </tr><tr>
        <td>
          390
        </td>
        <td>
          1394
        </td>
        <td>
          19715
          -
          19723
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.numRound
        </td>
        <td style="background: #AEF1AE">
          XGBoostParams.this.numRound
        </td>
      </tr><tr>
        <td>
          395
        </td>
        <td>
          1396
        </td>
        <td>
          19786
          -
          19795
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;xgboost&quot;
        </td>
      </tr><tr>
        <td>
          396
        </td>
        <td>
          1397
        </td>
        <td>
          19817
          -
          19820
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          &quot;1&quot;
        </td>
      </tr><tr>
        <td>
          397
        </td>
        <td>
          1398
        </td>
        <td>
          19846
          -
          19896
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.SagerMakerRegionAccountMaps.ApplicationsAccountMap
        </td>
        <td style="background: #AEF1AE">
          SagerMakerRegionAccountMaps.ApplicationsAccountMap
        </td>
      </tr><tr>
        <td>
          551
        </td>
        <td>
          1399
        </td>
        <td>
          29226
          -
          29233
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.booster
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.booster
        </td>
      </tr><tr>
        <td>
          551
        </td>
        <td>
          1400
        </td>
        <td>
          29222
          -
          29241
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[String](XGBoostSageMakerEstimator.this.booster, value)
        </td>
      </tr><tr>
        <td>
          553
        </td>
        <td>
          1402
        </td>
        <td>
          29285
          -
          29303
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Int](XGBoostSageMakerEstimator.this.silent, value)
        </td>
      </tr><tr>
        <td>
          553
        </td>
        <td>
          1401
        </td>
        <td>
          29289
          -
          29295
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.silent
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.silent
        </td>
      </tr><tr>
        <td>
          555
        </td>
        <td>
          1404
        </td>
        <td>
          29348
          -
          29367
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Int](XGBoostSageMakerEstimator.this.nThread, value)
        </td>
      </tr><tr>
        <td>
          555
        </td>
        <td>
          1403
        </td>
        <td>
          29352
          -
          29359
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.nThread
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.nThread
        </td>
      </tr><tr>
        <td>
          557
        </td>
        <td>
          1405
        </td>
        <td>
          29415
          -
          29418
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.eta
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.eta
        </td>
      </tr><tr>
        <td>
          557
        </td>
        <td>
          1406
        </td>
        <td>
          29411
          -
          29426
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.eta, value)
        </td>
      </tr><tr>
        <td>
          559
        </td>
        <td>
          1408
        </td>
        <td>
          29472
          -
          29489
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.gamma, value)
        </td>
      </tr><tr>
        <td>
          559
        </td>
        <td>
          1407
        </td>
        <td>
          29476
          -
          29481
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.gamma
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.gamma
        </td>
      </tr><tr>
        <td>
          561
        </td>
        <td>
          1410
        </td>
        <td>
          29538
          -
          29558
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.maxDepth, value)
        </td>
      </tr><tr>
        <td>
          561
        </td>
        <td>
          1409
        </td>
        <td>
          29542
          -
          29550
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.maxDepth
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.maxDepth
        </td>
      </tr><tr>
        <td>
          563
        </td>
        <td>
          1411
        </td>
        <td>
          29617
          -
          29631
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.minChildWeight
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.minChildWeight
        </td>
      </tr><tr>
        <td>
          563
        </td>
        <td>
          1412
        </td>
        <td>
          29613
          -
          29639
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.minChildWeight, value)
        </td>
      </tr><tr>
        <td>
          565
        </td>
        <td>
          1414
        </td>
        <td>
          29692
          -
          29716
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.maxDeltaStep, value)
        </td>
      </tr><tr>
        <td>
          565
        </td>
        <td>
          1413
        </td>
        <td>
          29696
          -
          29708
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.maxDeltaStep
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.maxDeltaStep
        </td>
      </tr><tr>
        <td>
          567
        </td>
        <td>
          1416
        </td>
        <td>
          29766
          -
          29787
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.subsample, value)
        </td>
      </tr><tr>
        <td>
          567
        </td>
        <td>
          1415
        </td>
        <td>
          29770
          -
          29779
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.subsample
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.subsample
        </td>
      </tr><tr>
        <td>
          569
        </td>
        <td>
          1418
        </td>
        <td>
          29843
          -
          29870
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.colSampleByTree, value)
        </td>
      </tr><tr>
        <td>
          569
        </td>
        <td>
          1417
        </td>
        <td>
          29847
          -
          29862
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.colSampleByTree
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.colSampleByTree
        </td>
      </tr><tr>
        <td>
          571
        </td>
        <td>
          1420
        </td>
        <td>
          29927
          -
          29955
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.colSampleByLevel, value)
        </td>
      </tr><tr>
        <td>
          571
        </td>
        <td>
          1419
        </td>
        <td>
          29931
          -
          29947
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.colSampleByLevel
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.colSampleByLevel
        </td>
      </tr><tr>
        <td>
          573
        </td>
        <td>
          1422
        </td>
        <td>
          30002
          -
          30020
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.lambda, value)
        </td>
      </tr><tr>
        <td>
          573
        </td>
        <td>
          1421
        </td>
        <td>
          30006
          -
          30012
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.lambda
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.lambda
        </td>
      </tr><tr>
        <td>
          575
        </td>
        <td>
          1423
        </td>
        <td>
          30070
          -
          30075
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.alpha
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.alpha
        </td>
      </tr><tr>
        <td>
          575
        </td>
        <td>
          1424
        </td>
        <td>
          30066
          -
          30083
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.alpha, value)
        </td>
      </tr><tr>
        <td>
          577
        </td>
        <td>
          1425
        </td>
        <td>
          30138
          -
          30148
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.treeMethod
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.treeMethod
        </td>
      </tr><tr>
        <td>
          577
        </td>
        <td>
          1426
        </td>
        <td>
          30134
          -
          30156
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[String](XGBoostSageMakerEstimator.this.treeMethod, value)
        </td>
      </tr><tr>
        <td>
          579
        </td>
        <td>
          1428
        </td>
        <td>
          30206
          -
          30227
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.sketchEps, value)
        </td>
      </tr><tr>
        <td>
          579
        </td>
        <td>
          1427
        </td>
        <td>
          30210
          -
          30219
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.sketchEps
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.sketchEps
        </td>
      </tr><tr>
        <td>
          581
        </td>
        <td>
          1429
        </td>
        <td>
          30286
          -
          30300
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.scalePosWeight
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.scalePosWeight
        </td>
      </tr><tr>
        <td>
          581
        </td>
        <td>
          1430
        </td>
        <td>
          30282
          -
          30308
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.scalePosWeight, value)
        </td>
      </tr><tr>
        <td>
          583
        </td>
        <td>
          1432
        </td>
        <td>
          30356
          -
          30375
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[String](XGBoostSageMakerEstimator.this.updater, value)
        </td>
      </tr><tr>
        <td>
          583
        </td>
        <td>
          1431
        </td>
        <td>
          30360
          -
          30367
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.updater
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.updater
        </td>
      </tr><tr>
        <td>
          585
        </td>
        <td>
          1434
        </td>
        <td>
          30424
          -
          30447
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Int](XGBoostSageMakerEstimator.this.refreshLeaf, value)
        </td>
      </tr><tr>
        <td>
          585
        </td>
        <td>
          1433
        </td>
        <td>
          30428
          -
          30439
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.refreshLeaf
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.refreshLeaf
        </td>
      </tr><tr>
        <td>
          587
        </td>
        <td>
          1436
        </td>
        <td>
          30499
          -
          30522
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[String](XGBoostSageMakerEstimator.this.processType, value)
        </td>
      </tr><tr>
        <td>
          587
        </td>
        <td>
          1435
        </td>
        <td>
          30503
          -
          30514
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.processType
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.processType
        </td>
      </tr><tr>
        <td>
          589
        </td>
        <td>
          1438
        </td>
        <td>
          30573
          -
          30595
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[String](XGBoostSageMakerEstimator.this.growPolicy, value)
        </td>
      </tr><tr>
        <td>
          589
        </td>
        <td>
          1437
        </td>
        <td>
          30577
          -
          30587
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.growPolicy
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.growPolicy
        </td>
      </tr><tr>
        <td>
          591
        </td>
        <td>
          1440
        </td>
        <td>
          30642
          -
          30663
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Int](XGBoostSageMakerEstimator.this.maxLeaves, value)
        </td>
      </tr><tr>
        <td>
          591
        </td>
        <td>
          1439
        </td>
        <td>
          30646
          -
          30655
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.maxLeaves
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.maxLeaves
        </td>
      </tr><tr>
        <td>
          593
        </td>
        <td>
          1441
        </td>
        <td>
          30711
          -
          30717
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.maxBin
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.maxBin
        </td>
      </tr><tr>
        <td>
          593
        </td>
        <td>
          1442
        </td>
        <td>
          30707
          -
          30725
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Int](XGBoostSageMakerEstimator.this.maxBin, value)
        </td>
      </tr><tr>
        <td>
          595
        </td>
        <td>
          1443
        </td>
        <td>
          30780
          -
          30790
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.sampleType
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.sampleType
        </td>
      </tr><tr>
        <td>
          595
        </td>
        <td>
          1444
        </td>
        <td>
          30776
          -
          30798
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[String](XGBoostSageMakerEstimator.this.sampleType, value)
        </td>
      </tr><tr>
        <td>
          597
        </td>
        <td>
          1446
        </td>
        <td>
          30852
          -
          30877
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[String](XGBoostSageMakerEstimator.this.normalizeType, value)
        </td>
      </tr><tr>
        <td>
          597
        </td>
        <td>
          1445
        </td>
        <td>
          30856
          -
          30869
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.normalizeType
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.normalizeType
        </td>
      </tr><tr>
        <td>
          599
        </td>
        <td>
          1447
        </td>
        <td>
          30930
          -
          30938
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.rateDrop
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.rateDrop
        </td>
      </tr><tr>
        <td>
          599
        </td>
        <td>
          1448
        </td>
        <td>
          30926
          -
          30946
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.rateDrop, value)
        </td>
      </tr><tr>
        <td>
          601
        </td>
        <td>
          1450
        </td>
        <td>
          30991
          -
          31010
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Int](XGBoostSageMakerEstimator.this.oneDrop, value)
        </td>
      </tr><tr>
        <td>
          601
        </td>
        <td>
          1449
        </td>
        <td>
          30995
          -
          31002
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.oneDrop
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.oneDrop
        </td>
      </tr><tr>
        <td>
          603
        </td>
        <td>
          1452
        </td>
        <td>
          31059
          -
          31079
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.skipDrop, value)
        </td>
      </tr><tr>
        <td>
          603
        </td>
        <td>
          1451
        </td>
        <td>
          31063
          -
          31071
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.skipDrop
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.skipDrop
        </td>
      </tr><tr>
        <td>
          605
        </td>
        <td>
          1454
        </td>
        <td>
          31130
          -
          31152
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.lambdaBias, value)
        </td>
      </tr><tr>
        <td>
          605
        </td>
        <td>
          1453
        </td>
        <td>
          31134
          -
          31144
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.lambdaBias
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.lambdaBias
        </td>
      </tr><tr>
        <td>
          607
        </td>
        <td>
          1456
        </td>
        <td>
          31213
          -
          31245
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.tweedieVariancePower, value)
        </td>
      </tr><tr>
        <td>
          607
        </td>
        <td>
          1455
        </td>
        <td>
          31217
          -
          31237
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.tweedieVariancePower
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.tweedieVariancePower
        </td>
      </tr><tr>
        <td>
          609
        </td>
        <td>
          1458
        </td>
        <td>
          31295
          -
          31316
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[String](XGBoostSageMakerEstimator.this.objective, value)
        </td>
      </tr><tr>
        <td>
          609
        </td>
        <td>
          1457
        </td>
        <td>
          31299
          -
          31308
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.objective
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.objective
        </td>
      </tr><tr>
        <td>
          611
        </td>
        <td>
          1459
        </td>
        <td>
          31368
          -
          31378
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.numClasses
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.numClasses
        </td>
      </tr><tr>
        <td>
          611
        </td>
        <td>
          1460
        </td>
        <td>
          31364
          -
          31386
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Int](XGBoostSageMakerEstimator.this.numClasses, value)
        </td>
      </tr><tr>
        <td>
          613
        </td>
        <td>
          1461
        </td>
        <td>
          31440
          -
          31449
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.baseScore
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.baseScore
        </td>
      </tr><tr>
        <td>
          613
        </td>
        <td>
          1462
        </td>
        <td>
          31436
          -
          31457
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.baseScore, value)
        </td>
      </tr><tr>
        <td>
          615
        </td>
        <td>
          1464
        </td>
        <td>
          31508
          -
          31530
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[String](XGBoostSageMakerEstimator.this.evalMetric, value)
        </td>
      </tr><tr>
        <td>
          615
        </td>
        <td>
          1463
        </td>
        <td>
          31512
          -
          31522
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.evalMetric
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.evalMetric
        </td>
      </tr><tr>
        <td>
          617
        </td>
        <td>
          1465
        </td>
        <td>
          31579
          -
          31583
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.seed
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.seed
        </td>
      </tr><tr>
        <td>
          617
        </td>
        <td>
          1466
        </td>
        <td>
          31575
          -
          31591
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Double](XGBoostSageMakerEstimator.this.seed, value)
        </td>
      </tr><tr>
        <td>
          619
        </td>
        <td>
          1468
        </td>
        <td>
          31637
          -
          31657
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.set
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.set[Int](XGBoostSageMakerEstimator.this.numRound, value)
        </td>
      </tr><tr>
        <td>
          619
        </td>
        <td>
          1467
        </td>
        <td>
          31641
          -
          31649
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.numRound
        </td>
        <td style="background: #AEF1AE">
          XGBoostSageMakerEstimator.this.numRound
        </td>
      </tr><tr>
        <td>
          623
        </td>
        <td>
          1470
        </td>
        <td>
          31783
          -
          31794
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.ml.param.Params.$
        </td>
        <td style="background: #F0ADAD">
          XGBoostSageMakerEstimator.this.$[Int](XGBoostSageMakerEstimator.this.numRound)
        </td>
      </tr><tr>
        <td>
          623
        </td>
        <td>
          1469
        </td>
        <td>
          31785
          -
          31793
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.algorithms.XGBoostParams.numRound
        </td>
        <td style="background: #F0ADAD">
          XGBoostSageMakerEstimator.this.numRound
        </td>
      </tr><tr>
        <td>
          624
        </td>
        <td>
          1471
        </td>
        <td>
          31799
          -
          31828
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator.transformSchema
        </td>
        <td style="background: #F0ADAD">
          XGBoostSageMakerEstimator.super.transformSchema(schema)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>