<html>
      <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
        <title id="title">
          com/amazonaws/services/sagemaker/sparksdk/transformation/util/RequestBatchIterator.scala.html
        </title>
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/css/theme.default.min.css" type="text/css"/><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tablesorter/2.20.1/js/jquery.tablesorter.min.js"></script><link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css" type="text/css"/><script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.3/js/bootstrap.min.js"></script><script type="text/javascript">
        $(document).ready(function() {$(".tablesorter").tablesorter();});
      </script>
        <style>
          table.codegrid { font-family: monospace; font-size: 12px; width: auto!important; }table.statementlist { width: auto!important; font-size: 13px; } table.codegrid td { padding: 0!important; border: 0!important } table td.linenumber { width: 40px!important; } 
        </style>
      </head>
      <body style="font-family: monospace;">
        <ul class="nav nav-tabs">
          <li>
            <a href="#codegrid" data-toggle="tab">Codegrid</a>
          </li>
          <li>
            <a href="#statementlist" data-toggle="tab">Statement List</a>
          </li>
        </ul>
        <div class="tab-content">
          <div class="tab-pane active" id="codegrid">
            <pre style='font-size: 12pt; font-family: courier;'>1 <span style=''>/*
</span>2 <span style=''> * Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.
</span>3 <span style=''> *
</span>4 <span style=''> * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;).
</span>5 <span style=''> * You may not use this file except in compliance with the License.
</span>6 <span style=''> * A copy of the License is located at
</span>7 <span style=''> *
</span>8 <span style=''> *   http://aws.amazon.com/apache2.0/
</span>9 <span style=''> *
</span>10 <span style=''> * or in the &quot;license&quot; file accompanying this file. This file is distributed
</span>11 <span style=''> * on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
</span>12 <span style=''> * express or implied. See the License for the specific language governing
</span>13 <span style=''> * permissions and limitations under the License.
</span>14 <span style=''> */
</span>15 <span style=''>
</span>16 <span style=''>package com.amazonaws.services.sagemaker.sparksdk.transformation.util
</span>17 <span style=''>
</span>18 <span style=''>import java.nio.ByteBuffer
</span>19 <span style=''>
</span>20 <span style=''>import scala.collection.mutable
</span>21 <span style=''>import scala.collection.mutable.ListBuffer
</span>22 <span style=''>
</span>23 <span style=''>import com.amazonaws.services.sagemakerruntime.{AmazonSageMakerRuntime, AmazonSageMakerRuntimeClientBuilder}
</span>24 <span style=''>import com.amazonaws.services.sagemakerruntime.model.InvokeEndpointRequest
</span>25 <span style=''>import com.amazonaws.util.BinaryUtils
</span>26 <span style=''>
</span>27 <span style=''>import org.apache.spark.sql.Row
</span>28 <span style=''>import org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema
</span>29 <span style=''>import org.apache.spark.sql.types.StructType
</span>30 <span style=''>
</span>31 <span style=''>import com.amazonaws.services.sagemaker.sparksdk.transformation.{RequestRowSerializer, ResponseRowDeserializer}
</span>32 <span style=''>
</span>33 <span style=''>object RequestBatchIterator {
</span>34 <span style=''>  var sagemakerRuntime : AmazonSageMakerRuntime =
</span>35 <span style=''>    </span><span style='background: #AEF1AE'>AmazonSageMakerRuntimeClientBuilder.defaultClient</span><span style=''>
</span>36 <span style=''>}
</span>37 <span style=''>
</span>38 <span style=''>/**
</span>39 <span style=''>  * Iterates over SageMaker transformed Rows by transforming input [[Row]]s to output Rows using
</span>40 <span style=''>  * a SageMaker Endpoint.
</span>41 <span style=''>  *
</span>42 <span style=''>  * SageMaker Transformation is done in batch. Input records are read from sourceIterator and
</span>43 <span style=''>  * serialized to an Array[Byte] using a [[RequestRowSerializer]]. These byte arrays are
</span>44 <span style=''>  * concatenated into a batch request containing at most maxBatchSizeInRecords records and at
</span>45 <span style=''>  * most maxBatchSizeInBytes number of bytes. Transformation is then performed by invoking
</span>46 <span style=''>  * [[AmazonSageMakerRuntime#invokeEndpoint]] on an [[AmazonSageMakerRuntime]], using the Byte
</span>47 <span style=''>  * Array as the request body.
</span>48 <span style=''>  *
</span>49 <span style=''>  * The transformation response as an Array of Bytes is converted to a series of Rows
</span>50 <span style=''>  * by a [[ResponseRowDeserializer]]
</span>51 <span style=''>  *
</span>52 <span style=''>  * The SageMaker transformation uses a content type retrieved from
</span>53 <span style=''>  * [[RequestRowSerializer#contentType]] and an accepts from [[ResponseRowDeserializer#accepts]].
</span>54 <span style=''>  * The SageMaker transformation response is converted to an Iterator of Rows by
</span>55 <span style=''>  * responseRowDeserializer.
</span>56 <span style=''>  *
</span>57 <span style=''>  * @param endpointName The name of the SageMaker endpoint to invoke
</span>58 <span style=''>  * @param sourceIterator Input Rows are read from this iterator
</span>59 <span style=''>  * @param requestRowSerializer Serializes each Row to a Byte Array for SageMaker transformation
</span>60 <span style=''>  * @param responseRowDeserializer Deserializes each SageMaker Endpoint transformation response to a
</span>61 <span style=''>  *                                series of Rows
</span>62 <span style=''>  * @param prependResultRows Whether output Rows returned by this Iterator should be prepended with
</span>63 <span style=''>  *                          the input Rows read from sourceIter
</span>64 <span style=''>  * @param maxBatchSizeInRecords The maximum number of records to send in each batch to the
</span>65 <span style=''>  *                              SageMaker endpoint
</span>66 <span style=''>  * @param maxBatchSizeInBytes The maximum byte size to send in each batch to the SageMaker endpoint
</span>67 <span style=''>  */
</span>68 <span style=''> private[sparksdk] class RequestBatchIterator (
</span>69 <span style=''>                                              val endpointName : String,
</span>70 <span style=''>                                              val sourceIterator : Iterator[Row],
</span>71 <span style=''>                                              val requestRowSerializer: RequestRowSerializer,
</span>72 <span style=''>                                              val responseRowDeserializer: ResponseRowDeserializer,
</span>73 <span style=''>                                              val prependResultRows : Boolean = true,
</span>74 <span style=''>                                              val maxBatchSizeInRecords : Int = Int.MaxValue,
</span>75 <span style=''>                                              val maxBatchSizeInBytes : Int = 5242880)
</span>76 <span style=''>  extends Iterator[Row] {
</span>77 <span style=''>
</span>78 <span style=''>  if (</span><span style='background: #AEF1AE'>maxBatchSizeInRecords &lt; 1</span><span style=''>) {
</span>79 <span style=''>    </span><span style='background: #AEF1AE'>throw new IllegalArgumentException(&quot;maxBatchSizeInRecords &lt; 1&quot;)</span><span style=''>
</span>80 <span style=''>  }
</span>81 <span style=''>
</span>82 <span style=''>  if (</span><span style='background: #AEF1AE'>maxBatchSizeInBytes &lt; 1</span><span style=''>) {
</span>83 <span style=''>    </span><span style='background: #AEF1AE'>throw new IllegalArgumentException(&quot;maxBatchSizeInBytes &lt; 1&quot;)</span><span style=''>
</span>84 <span style=''>  }
</span>85 <span style=''>
</span>86 <span style=''>  /**
</span>87 <span style=''>    * Stores transformed records to return from this iterator.
</span>88 <span style=''>    */
</span>89 <span style=''>  private var currentResultsIterator = </span><span style='background: #AEF1AE'>ListBuffer.empty[Row].iterator</span><span style=''>
</span>90 <span style=''>
</span>91 <span style=''>  /**
</span>92 <span style=''>    * A convenience wrapper for sourceIterator and requestObjectSerializer.
</span>93 <span style=''>    * Converts the sourceIterator into an Iterator[Array[Byte]].
</span>94 <span style=''>    */
</span>95 <span style=''>  private val requestObjectIterator = </span><span style='background: #AEF1AE'>new RequestRowIterator(
</span>96 <span style=''></span><span style='background: #AEF1AE'>    sourceIterator,
</span>97 <span style=''></span><span style='background: #AEF1AE'>    maxBatchSizeInBytes,
</span>98 <span style=''></span><span style='background: #AEF1AE'>    requestRowSerializer)</span><span style=''>
</span>99 <span style=''>
</span>100 <span style=''>  override def hasNext : Boolean = {
</span>101 <span style=''>    // Iteratively construct currentResultsIterator from SageMaker requests until
</span>102 <span style=''>    // a currentResultsIterator exists with at least one record to return.
</span>103 <span style=''>    while (</span><span style='background: #AEF1AE'>!currentResultsIterator.hasNext</span><span style=''>) </span><span style='background: #AEF1AE'>{
</span>104 <span style=''></span><span style='background: #AEF1AE'>      if (!requestObjectIterator.hasNext) { // Return false if no input records left to transform
</span>105 <span style=''></span><span style='background: #AEF1AE'>        return false
</span>106 <span style=''></span><span style='background: #AEF1AE'>      }
</span>107 <span style=''></span><span style='background: #AEF1AE'>      // Construct a SageMaker request as an array of bytes by taking objects from the
</span>108 <span style=''></span><span style='background: #AEF1AE'>      // sourceIter and serializing hem with requestObjectSerializer
</span>109 <span style=''></span><span style='background: #AEF1AE'>      var batchSizeInRecords = 0
</span>110 <span style=''></span><span style='background: #AEF1AE'>      val batchBody = ByteBuffer.allocate(maxBatchSizeInBytes)
</span>111 <span style=''></span><span style='background: #AEF1AE'>      val batchInputRows = new mutable.ListBuffer[Row]()
</span>112 <span style=''></span><span style='background: #AEF1AE'>      while(requestObjectIterator.hasNext &amp;&amp;
</span>113 <span style=''></span><span style='background: #AEF1AE'>        requestObjectIterator.nextValueLength + batchBody.position &lt;= maxBatchSizeInBytes &amp;&amp;
</span>114 <span style=''></span><span style='background: #AEF1AE'>        batchSizeInRecords &lt;= maxBatchSizeInRecords) {
</span>115 <span style=''></span><span style='background: #AEF1AE'>        val (nextRow, nextSerializedRow) = requestObjectIterator.next()
</span>116 <span style=''></span><span style='background: #AEF1AE'>        if(prependResultRows) {
</span>117 <span style=''></span><span style='background: #AEF1AE'>          batchInputRows += nextRow
</span>118 <span style=''></span><span style='background: #AEF1AE'>        }
</span>119 <span style=''></span><span style='background: #AEF1AE'>        batchBody.put(nextSerializedRow)
</span>120 <span style=''></span><span style='background: #AEF1AE'>        batchSizeInRecords += 1
</span>121 <span style=''></span><span style='background: #AEF1AE'>      }
</span>122 <span style=''></span><span style='background: #AEF1AE'>      batchBody.flip()
</span>123 <span style=''></span><span style='background: #AEF1AE'>      val invokeEndpointRequest = new InvokeEndpointRequest()
</span>124 <span style=''></span><span style='background: #AEF1AE'>        .withEndpointName(endpointName)
</span>125 <span style=''></span><span style='background: #AEF1AE'>        .withContentType(requestRowSerializer.contentType)
</span>126 <span style=''></span><span style='background: #AEF1AE'>        .withAccept(responseRowDeserializer.accepts)
</span>127 <span style=''></span><span style='background: #AEF1AE'>        .withBody(batchBody)
</span>128 <span style=''></span><span style='background: #AEF1AE'>      val endpointResponse = RequestBatchIterator.sagemakerRuntime
</span>129 <span style=''></span><span style='background: #AEF1AE'>        .invokeEndpoint(invokeEndpointRequest).getBody
</span>130 <span style=''></span><span style='background: #AEF1AE'>
</span>131 <span style=''></span><span style='background: #AEF1AE'>      val resultsIterator = responseRowDeserializer
</span>132 <span style=''></span><span style='background: #AEF1AE'>          .deserializeResponse(BinaryUtils.copyBytesFrom(endpointResponse))
</span>133 <span style=''></span><span style='background: #AEF1AE'>
</span>134 <span style=''></span><span style='background: #AEF1AE'>      // If result rows should have input rows prepended, then wrap the resultsIterator from
</span>135 <span style=''></span><span style='background: #AEF1AE'>      // the deserializer with an iterator that does the prepending
</span>136 <span style=''></span><span style='background: #AEF1AE'>      if (prependResultRows) {
</span>137 <span style=''></span><span style='background: #AEF1AE'>        currentResultsIterator = new Iterator[Row] {
</span>138 <span style=''></span><span style='background: #AEF1AE'>          val batchInputRowsIterator = batchInputRows.iterator.buffered
</span>139 <span style=''></span><span style='background: #AEF1AE'>          val row = batchInputRowsIterator.head
</span>140 <span style=''></span><span style='background: #AEF1AE'>          val schema = row.schema
</span>141 <span style=''></span><span style='background: #AEF1AE'>          assert(schema != null,
</span>142 <span style=''></span><span style='background: #AEF1AE'>            </span><span style='background: #F0ADAD'>&quot;Input rows must have a schema when prepending input rows to result rows&quot;</span><span style='background: #AEF1AE'>)
</span>143 <span style=''></span><span style='background: #AEF1AE'>          val newSchema = StructType(batchInputRowsIterator.head.schema
</span>144 <span style=''></span><span style='background: #AEF1AE'>            ++ responseRowDeserializer.schema)
</span>145 <span style=''></span><span style='background: #AEF1AE'>          override def hasNext: Boolean = resultsIterator.hasNext
</span>146 <span style=''></span><span style='background: #AEF1AE'>          override def next(): Row = {
</span>147 <span style=''></span><span style='background: #AEF1AE'>            new GenericRowWithSchema(
</span>148 <span style=''></span><span style='background: #AEF1AE'>              (batchInputRowsIterator.next.toSeq ++ resultsIterator.next.toSeq).toArray,
</span>149 <span style=''></span><span style='background: #AEF1AE'>              newSchema)
</span>150 <span style=''></span><span style='background: #AEF1AE'>          }
</span>151 <span style=''></span><span style='background: #AEF1AE'>        }
</span>152 <span style=''></span><span style='background: #AEF1AE'>      } else {
</span>153 <span style=''></span><span style='background: #AEF1AE'>        currentResultsIterator = resultsIterator
</span>154 <span style=''></span><span style='background: #AEF1AE'>      }
</span>155 <span style=''></span><span style='background: #AEF1AE'>    }</span><span style=''>
</span>156 <span style=''>    </span><span style='background: #AEF1AE'>true</span><span style=''>
</span>157 <span style=''>  }
</span>158 <span style=''>
</span>159 <span style=''>  override def next() : Row = {
</span>160 <span style=''>    if (</span><span style='background: #AEF1AE'>currentResultsIterator.hasNext</span><span style=''>) {
</span>161 <span style=''>      </span><span style='background: #AEF1AE'>return currentResultsIterator.next</span><span style=''>
</span>162 <span style=''>    }
</span>163 <span style=''>    if (</span><span style='background: #AEF1AE'>!hasNext</span><span style=''>) {
</span>164 <span style=''>      </span><span style='background: #AEF1AE'>throw new NoSuchElementException</span><span style=''>
</span>165 <span style=''>    }
</span>166 <span style=''>    </span><span style='background: #AEF1AE'>currentResultsIterator.next</span><span style=''>
</span>167 <span style=''>  }
</span>168 <span style=''>}
</span>169 <span style=''>
</span>170 <span style=''>/**
</span>171 <span style=''>  * Translates an Iterator[Row] to an Iterator[(Row, Array[Byte])] via a RequestObjectSerializer[T].
</span>172 <span style=''>  */
</span>173 <span style=''>private class RequestRowIterator (val input: Iterator[Row],
</span>174 <span style=''>                                  val maxBatchSizeInBytes : Int,
</span>175 <span style=''>                                  val requestObjectSerializer: RequestRowSerializer)
</span>176 <span style=''>  extends Iterator[(Row, Array[Byte])] {
</span>177 <span style=''>
</span>178 <span style=''>  // Facilitate peeking by eagerly reading the first value from sourceIterator, if it exists
</span>179 <span style=''>  var nextValue : Option[(Row, Array[Byte])] = </span><span style='background: #AEF1AE'>Option.empty</span><span style=''>
</span>180 <span style=''>
</span>181 <span style=''>  if (</span><span style='background: #AEF1AE'>input.hasNext</span><span style=''>) {
</span>182 <span style=''>    </span><span style='background: #AEF1AE'>nextValue = Option(serializeRow(input.next))</span><span style=''>
</span>183 <span style=''>  }
</span>184 <span style=''>
</span>185 <span style=''>  /**
</span>186 <span style=''>    * Returns the length in bytes of the next serialized row or 0 if there is no next row
</span>187 <span style=''>    */
</span>188 <span style=''>  def nextValueLength : Int = {
</span>189 <span style=''>    </span><span style='background: #AEF1AE'>nextValue map {case (a, b) =&gt; b.length} getOrElse 0</span><span style=''>
</span>190 <span style=''>  }
</span>191 <span style=''>
</span>192 <span style=''>  override def hasNext: Boolean = {
</span>193 <span style=''>    </span><span style='background: #AEF1AE'>nextValue.nonEmpty</span><span style=''>
</span>194 <span style=''>  }
</span>195 <span style=''>
</span>196 <span style=''>  override def next(): (Row, Array[Byte]) = {
</span>197 <span style=''>    val returnVal = </span><span style='background: #AEF1AE'>nextValue.get</span><span style=''>
</span>198 <span style=''>    if(</span><span style='background: #AEF1AE'>input.hasNext</span><span style=''>) {
</span>199 <span style=''>      </span><span style='background: #AEF1AE'>nextValue = Option(serializeRow(input.next))</span><span style=''>
</span>200 <span style=''>    } else {
</span>201 <span style=''>      </span><span style='background: #AEF1AE'>nextValue = Option.empty</span><span style=''>
</span>202 <span style=''>    }
</span>203 <span style=''>    returnVal
</span>204 <span style=''>  }
</span>205 <span style=''>
</span>206 <span style=''>  private def serializeRow(obj : Row): (Row, Array[Byte]) = {
</span>207 <span style=''>    val serialized = </span><span style='background: #AEF1AE'>requestObjectSerializer.serializeRow(obj)</span><span style=''>
</span>208 <span style=''>    if (</span><span style='background: #AEF1AE'>serialized.length &gt; maxBatchSizeInBytes</span><span style=''>) {
</span>209 <span style=''>      </span><span style='background: #AEF1AE'>throw new IllegalArgumentException(&quot;Object serialized to byte array of length: &quot;
</span>210 <span style=''></span><span style='background: #AEF1AE'>        + serialized.length + &quot; which is above max batch size in bytes of &quot;
</span>211 <span style=''></span><span style='background: #AEF1AE'>        + maxBatchSizeInBytes + &quot;. Object: &quot; + obj)</span><span style=''>
</span>212 <span style=''>    }
</span>213 <span style=''>    </span><span style='background: #AEF1AE'>(obj, serialized)</span><span style=''>
</span>214 <span style=''>  }
</span>215 <span style=''>}
</span>216 <span style=''>
</span>217 <span style=''>object RequestBatchIteratorFactory extends Serializable {
</span>218 <span style=''>
</span>219 <span style=''>  /**
</span>220 <span style=''>    * Creates [[RequestBatchIterator]] for SageMakerModel.
</span>221 <span style=''>    *
</span>222 <span style=''>    * @param endpointName The name of an endpoint that is current in service.
</span>223 <span style=''>    * @param ser Serializes a Row to an Array of Bytes.
</span>224 <span style=''>    * @param de Deserializes an Array of Bytes to a series of Rows.
</span>225 <span style=''>    * @param prependResultRows Whether the transformation result should include the input Rows.
</span>226 <span style=''>    * @return An iterator.
</span>227 <span style=''>    */
</span>228 <span style=''>  def createRequestBatchIterator(endpointName : String,
</span>229 <span style=''>                                 ser : RequestRowSerializer,
</span>230 <span style=''>                                 de : ResponseRowDeserializer,
</span>231 <span style=''>                                 prependResultRows : Boolean):
</span>232 <span style=''>  Iterator[Row] =&gt; RequestBatchIterator = (input : Iterator[Row]) =&gt; {
</span>233 <span style=''>    </span><span style='background: #F0ADAD'>new RequestBatchIterator(
</span>234 <span style=''></span><span style='background: #F0ADAD'>      endpointName,
</span>235 <span style=''></span><span style='background: #F0ADAD'>      input,
</span>236 <span style=''></span><span style='background: #F0ADAD'>      ser,
</span>237 <span style=''></span><span style='background: #F0ADAD'>      de,
</span>238 <span style=''></span><span style='background: #F0ADAD'>      prependResultRows)</span><span style=''>
</span>239 <span style=''>  }
</span>240 <span style=''>}
</span></pre>
          </div>
          <div class="tab-pane" id="statementlist">
            <table cellspacing="0" cellpadding="0" class="table statementlist">
      <tr>
        <th>Line</th>
        <th>Stmt Id</th>
        <th>Pos</th>
        <th>Tree</th>
        <th>Symbol</th>
        <th>Code</th>
      </tr><tr>
        <td>
          35
        </td>
        <td>
          1942
        </td>
        <td>
          1326
          -
          1375
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClientBuilder.defaultClient
        </td>
        <td style="background: #AEF1AE">
          com.amazonaws.services.sagemakerruntime.AmazonSageMakerRuntimeClientBuilder.defaultClient()
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          1943
        </td>
        <td>
          3909
          -
          3934
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.maxBatchSizeInRecords.&lt;(1)
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          1946
        </td>
        <td>
          3905
          -
          3905
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          78
        </td>
        <td>
          1947
        </td>
        <td>
          3905
          -
          3905
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          1945
        </td>
        <td>
          3942
          -
          4005
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          throw new scala.`package`.IllegalArgumentException(&quot;maxBatchSizeInRecords &lt; 1&quot;)
        </td>
      </tr><tr>
        <td>
          79
        </td>
        <td>
          1944
        </td>
        <td>
          3942
          -
          4005
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          throw new scala.`package`.IllegalArgumentException(&quot;maxBatchSizeInRecords &lt; 1&quot;)
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          1952
        </td>
        <td>
          4013
          -
          4013
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          1951
        </td>
        <td>
          4013
          -
          4013
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          82
        </td>
        <td>
          1948
        </td>
        <td>
          4017
          -
          4040
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.maxBatchSizeInBytes.&lt;(1)
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          1949
        </td>
        <td>
          4048
          -
          4109
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          throw new scala.`package`.IllegalArgumentException(&quot;maxBatchSizeInBytes &lt; 1&quot;)
        </td>
      </tr><tr>
        <td>
          83
        </td>
        <td>
          1950
        </td>
        <td>
          4048
          -
          4109
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          throw new scala.`package`.IllegalArgumentException(&quot;maxBatchSizeInBytes &lt; 1&quot;)
        </td>
      </tr><tr>
        <td>
          89
        </td>
        <td>
          1953
        </td>
        <td>
          4230
          -
          4260
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.mutable.ListBuffer.iterator
        </td>
        <td style="background: #AEF1AE">
          scala.collection.mutable.ListBuffer.empty[org.apache.spark.sql.Row].iterator
        </td>
      </tr><tr>
        <td>
          95
        </td>
        <td>
          1957
        </td>
        <td>
          4454
          -
          4548
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new RequestRowIterator(RequestBatchIterator.this.sourceIterator, RequestBatchIterator.this.maxBatchSizeInBytes, RequestBatchIterator.this.requestRowSerializer)
        </td>
      </tr><tr>
        <td>
          96
        </td>
        <td>
          1954
        </td>
        <td>
          4482
          -
          4496
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.sourceIterator
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.sourceIterator
        </td>
      </tr><tr>
        <td>
          97
        </td>
        <td>
          1955
        </td>
        <td>
          4502
          -
          4521
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.maxBatchSizeInBytes
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.maxBatchSizeInBytes
        </td>
      </tr><tr>
        <td>
          98
        </td>
        <td>
          1956
        </td>
        <td>
          4527
          -
          4547
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.requestRowSerializer
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.requestRowSerializer
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          2015
        </td>
        <td>
          4788
          -
          7162
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  {
    if (RequestBatchIterator.this.requestObjectIterator.hasNext.unary_!)
      return false
    else
      ();
    var batchSizeInRecords: Int = 0;
    val batchBody: java.nio.ByteBuffer = java.nio.ByteBuffer.allocate(RequestBatchIterator.this.maxBatchSizeInBytes);
    val batchInputRows: scala.collection.mutable.ListBuffer[org.apache.spark.sql.Row] = new scala.collection.mutable.ListBuffer[org.apache.spark.sql.Row]();
    while$1(){
      if (RequestBatchIterator.this.requestObjectIterator.hasNext.&amp;&amp;(RequestBatchIterator.this.requestObjectIterator.nextValueLength.+(batchBody.position()).&lt;=(RequestBatchIterator.this.maxBatchSizeInBytes)).&amp;&amp;(batchSizeInRecords.&lt;=(RequestBatchIterator.this.maxBatchSizeInRecords)))
        {
          {
            &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$1: (org.apache.spark.sql.Row, Array[Byte]) = (RequestBatchIterator.this.requestObjectIterator.next(): (org.apache.spark.sql.Row, Array[Byte]) @unchecked) match {
              case (_1: org.apache.spark.sql.Row, _2: Array[Byte])(org.apache.spark.sql.Row, Array[Byte])((nextRow @ _), (nextSerializedRow @ _)) =&gt; scala.Tuple2.apply[org.apache.spark.sql.Row, Array[Byte]](nextRow, nextSerializedRow)
            };
            val nextRow: org.apache.spark.sql.Row = x$1._1;
            val nextSerializedRow: Array[Byte] = x$1._2;
            if (RequestBatchIterator.this.prependResultRows)
              batchInputRows.+=(nextRow)
            else
              ();
            batchBody.put(nextSerializedRow);
            batchSizeInRecords = batchSizeInRecords.+(1)
          };
          while$1()
        }
      else
        ()
    };
    batchBody.flip();
    val invokeEndpointRequest: com.amazonaws.services.sagemakerruntime.model.InvokeEndpointRequest = new com.amazonaws.services.sagemakerruntime.model.InvokeEndpointRequest().withEndpointName(RequestBatchIterator.this.endpointName).withContentType(RequestBatchIterator.this.requestRowSerializer.contentType).withAccept(RequestBatchIterator.this.responseRowDeserializer.accepts).withBody(batchBody);
    val endpointResponse: java.nio.ByteBuffer = RequestBatchIterator.sagemakerRuntime.invokeEndpoint(invokeEndpointRequest).getBody();
    val resultsIterator: Iterator[org.apache.spark.sql.Row] = RequestBatchIterator.this.responseRowDeserializer.deserializeResponse(com.amazonaws.util.BinaryUtils.copyBytesFrom(endpointResponse));
    if (RequestBatchIterator.this.prependResultRows)
      RequestBatchIterator.this.currentResultsIterator_=({
        final class $anon extends AnyRef with Iterator[org.apache.spark.sql.Row] {
          def &lt;init&gt;(): &lt;$anon: Iterator[org.apache.spark.sql.Row]&gt; = {
            $anon.super.&lt;init&gt;();
            ()
          };
          private[this] val batchInputRowsIterator: scala.collection.BufferedIterator[org.apache.spark.sql.Row] = batchInputRows.iterator.buffered;
          &lt;stable&gt; &lt;accessor&gt; private def batchInputRowsIterator: scala.collection.BufferedIterator[org.apache.spark.sql.Row] = $anon.this.batchInputRowsIterator;
          private[this] val row: org.apache.spark.sql.Row = $anon.this.batchInputRowsIterator.head;
          &lt;stable&gt; &lt;accessor&gt; private def row: org.apache.spark.sql.Row = $anon.this.row;
          private[this] val schema: org.apache.spark.sql.types.StructType = $anon.this.row.schema;
          &lt;stable&gt; &lt;accessor&gt; private def schema: org.apache.spark.sql.types.StructType = $anon.this.schema;
          scala.this.Predef.assert($anon.this.schema.!=(null), &quot;Input rows must have a schema when prepending input rows to result rows&quot;);
          private[this] val newSchema: org.apache.spark.sql.types.StructType = org.apache.spark.sql.types.StructType.apply($anon.this.batchInputRowsIterator.head.schema.++[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](RequestBatchIterator.this.responseRowDeserializer.schema)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]));
          &lt;stable&gt; &lt;accessor&gt; private def newSchema: org.apache.spark.sql.types.StructType = $anon.this.newSchema;
          override def hasNext: Boolean = resultsIterator.hasNext;
          override def next(): org.apache.spark.sql.Row = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema($anon.this.batchInputRowsIterator.next().toSeq.++[Any, Seq[Any]](resultsIterator.next().toSeq)(collection.this.Seq.canBuildFrom[Any]).toArray[Any]((ClassTag.Any: scala.reflect.ClassTag[Any])), $anon.this.newSchema)
        };
        new $anon()
      })
    else
      RequestBatchIterator.this.currentResultsIterator_=(resultsIterator)
  };
  while$2()
}
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          1958
        </td>
        <td>
          4755
          -
          4786
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.currentResultsIterator.hasNext.unary_!
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          2017
        </td>
        <td>
          4748
          -
          4748
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          2014
        </td>
        <td>
          4788
          -
          4788
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.while$2
        </td>
        <td style="background: #AEF1AE">
          while$2()
        </td>
      </tr><tr>
        <td>
          103
        </td>
        <td>
          2016
        </td>
        <td>
          4748
          -
          4748
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          1963
        </td>
        <td>
          4796
          -
          4796
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          1959
        </td>
        <td>
          4800
          -
          4830
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.requestObjectIterator.hasNext.unary_!
        </td>
      </tr><tr>
        <td>
          104
        </td>
        <td>
          1962
        </td>
        <td>
          4796
          -
          4796
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          105
        </td>
        <td>
          1961
        </td>
        <td>
          4896
          -
          4908
        </td>
        <td>
          Return
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.hasNext
        </td>
        <td style="background: #AEF1AE">
          return false
        </td>
      </tr><tr>
        <td>
          105
        </td>
        <td>
          1960
        </td>
        <td>
          4903
          -
          4908
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          false
        </td>
      </tr><tr>
        <td>
          109
        </td>
        <td>
          1964
        </td>
        <td>
          5104
          -
          5105
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          0
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          1966
        </td>
        <td>
          5128
          -
          5168
        </td>
        <td>
          Apply
        </td>
        <td>
          java.nio.ByteBuffer.allocate
        </td>
        <td style="background: #AEF1AE">
          java.nio.ByteBuffer.allocate(RequestBatchIterator.this.maxBatchSizeInBytes)
        </td>
      </tr><tr>
        <td>
          110
        </td>
        <td>
          1965
        </td>
        <td>
          5148
          -
          5167
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.maxBatchSizeInBytes
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.maxBatchSizeInBytes
        </td>
      </tr><tr>
        <td>
          111
        </td>
        <td>
          1967
        </td>
        <td>
          5196
          -
          5225
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.ListBuffer.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new scala.collection.mutable.ListBuffer[org.apache.spark.sql.Row]()
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          1985
        </td>
        <td>
          5232
          -
          5232
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          1984
        </td>
        <td>
          5417
          -
          5649
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          {
  {
    &lt;synthetic&gt; &lt;artifact&gt; private[this] val x$1: (org.apache.spark.sql.Row, Array[Byte]) = (RequestBatchIterator.this.requestObjectIterator.next(): (org.apache.spark.sql.Row, Array[Byte]) @unchecked) match {
      case (_1: org.apache.spark.sql.Row, _2: Array[Byte])(org.apache.spark.sql.Row, Array[Byte])((nextRow @ _), (nextSerializedRow @ _)) =&gt; scala.Tuple2.apply[org.apache.spark.sql.Row, Array[Byte]](nextRow, nextSerializedRow)
    };
    val nextRow: org.apache.spark.sql.Row = x$1._1;
    val nextSerializedRow: Array[Byte] = x$1._2;
    if (RequestBatchIterator.this.prependResultRows)
      batchInputRows.+=(nextRow)
    else
      ();
    batchBody.put(nextSerializedRow);
    batchSizeInRecords = batchSizeInRecords.+(1)
  };
  while$1()
}
        </td>
      </tr><tr>
        <td>
          112
        </td>
        <td>
          1986
        </td>
        <td>
          5232
          -
          5232
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          1970
        </td>
        <td>
          5279
          -
          5360
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;=
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.requestObjectIterator.nextValueLength.+(batchBody.position()).&lt;=(RequestBatchIterator.this.maxBatchSizeInBytes)
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          1973
        </td>
        <td>
          5238
          -
          5415
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Boolean.&amp;&amp;
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.requestObjectIterator.hasNext.&amp;&amp;(RequestBatchIterator.this.requestObjectIterator.nextValueLength.+(batchBody.position()).&lt;=(RequestBatchIterator.this.maxBatchSizeInBytes)).&amp;&amp;(batchSizeInRecords.&lt;=(RequestBatchIterator.this.maxBatchSizeInRecords))
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          1969
        </td>
        <td>
          5341
          -
          5360
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.maxBatchSizeInBytes
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.maxBatchSizeInBytes
        </td>
      </tr><tr>
        <td>
          113
        </td>
        <td>
          1968
        </td>
        <td>
          5319
          -
          5337
        </td>
        <td>
          Apply
        </td>
        <td>
          java.nio.Buffer.position
        </td>
        <td style="background: #AEF1AE">
          batchBody.position()
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1972
        </td>
        <td>
          5372
          -
          5415
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&lt;=
        </td>
        <td style="background: #AEF1AE">
          batchSizeInRecords.&lt;=(RequestBatchIterator.this.maxBatchSizeInRecords)
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1971
        </td>
        <td>
          5394
          -
          5415
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.maxBatchSizeInRecords
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.maxBatchSizeInRecords
        </td>
      </tr><tr>
        <td>
          114
        </td>
        <td>
          1983
        </td>
        <td>
          5417
          -
          5417
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.while$1
        </td>
        <td style="background: #AEF1AE">
          while$1()
        </td>
      </tr><tr>
        <td>
          115
        </td>
        <td>
          1975
        </td>
        <td>
          5441
          -
          5441
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._2
        </td>
        <td style="background: #AEF1AE">
          x$1._2
        </td>
      </tr><tr>
        <td>
          115
        </td>
        <td>
          1974
        </td>
        <td>
          5432
          -
          5432
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Tuple2._1
        </td>
        <td style="background: #AEF1AE">
          x$1._1
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          1979
        </td>
        <td>
          5499
          -
          5499
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          1976
        </td>
        <td>
          5502
          -
          5519
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.prependResultRows
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.prependResultRows
        </td>
      </tr><tr>
        <td>
          116
        </td>
        <td>
          1980
        </td>
        <td>
          5499
          -
          5499
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1978
        </td>
        <td>
          5533
          -
          5558
        </td>
        <td>
          Block
        </td>
        <td>
          scala.collection.mutable.ListBuffer.+=
        </td>
        <td style="background: #AEF1AE">
          batchInputRows.+=(nextRow)
        </td>
      </tr><tr>
        <td>
          117
        </td>
        <td>
          1977
        </td>
        <td>
          5533
          -
          5558
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.mutable.ListBuffer.+=
        </td>
        <td style="background: #AEF1AE">
          batchInputRows.+=(nextRow)
        </td>
      </tr><tr>
        <td>
          119
        </td>
        <td>
          1981
        </td>
        <td>
          5577
          -
          5609
        </td>
        <td>
          Apply
        </td>
        <td>
          java.nio.ByteBuffer.put
        </td>
        <td style="background: #AEF1AE">
          batchBody.put(nextSerializedRow)
        </td>
      </tr><tr>
        <td>
          120
        </td>
        <td>
          1982
        </td>
        <td>
          5618
          -
          5641
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.+
        </td>
        <td style="background: #AEF1AE">
          batchSizeInRecords.+(1)
        </td>
      </tr><tr>
        <td>
          122
        </td>
        <td>
          1987
        </td>
        <td>
          5656
          -
          5672
        </td>
        <td>
          Apply
        </td>
        <td>
          java.nio.Buffer.flip
        </td>
        <td style="background: #AEF1AE">
          batchBody.flip()
        </td>
      </tr><tr>
        <td>
          127
        </td>
        <td>
          1988
        </td>
        <td>
          5707
          -
          5915
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemakerruntime.model.InvokeEndpointRequest.withBody
        </td>
        <td style="background: #AEF1AE">
          new com.amazonaws.services.sagemakerruntime.model.InvokeEndpointRequest().withEndpointName(RequestBatchIterator.this.endpointName).withContentType(RequestBatchIterator.this.requestRowSerializer.contentType).withAccept(RequestBatchIterator.this.responseRowDeserializer.accepts).withBody(batchBody)
        </td>
      </tr><tr>
        <td>
          129
        </td>
        <td>
          1989
        </td>
        <td>
          5945
          -
          6037
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemakerruntime.model.InvokeEndpointResult.getBody
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.sagemakerRuntime.invokeEndpoint(invokeEndpointRequest).getBody()
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1991
        </td>
        <td>
          6067
          -
          6166
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.ResponseRowDeserializer.deserializeResponse
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.responseRowDeserializer.deserializeResponse(com.amazonaws.util.BinaryUtils.copyBytesFrom(endpointResponse))
        </td>
      </tr><tr>
        <td>
          132
        </td>
        <td>
          1990
        </td>
        <td>
          6122
          -
          6165
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.util.BinaryUtils.copyBytesFrom
        </td>
        <td style="background: #AEF1AE">
          com.amazonaws.util.BinaryUtils.copyBytesFrom(endpointResponse)
        </td>
      </tr><tr>
        <td>
          136
        </td>
        <td>
          1992
        </td>
        <td>
          6339
          -
          6356
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.prependResultRows
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.prependResultRows
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          2009
        </td>
        <td>
          6393
          -
          6396
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.$anon.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new $anon()
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          2011
        </td>
        <td>
          6368
          -
          7084
        </td>
        <td>
          Block
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.currentResultsIterator_=
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.currentResultsIterator_=({
  final class $anon extends AnyRef with Iterator[org.apache.spark.sql.Row] {
    def &lt;init&gt;(): &lt;$anon: Iterator[org.apache.spark.sql.Row]&gt; = {
      $anon.super.&lt;init&gt;();
      ()
    };
    private[this] val batchInputRowsIterator: scala.collection.BufferedIterator[org.apache.spark.sql.Row] = batchInputRows.iterator.buffered;
    &lt;stable&gt; &lt;accessor&gt; private def batchInputRowsIterator: scala.collection.BufferedIterator[org.apache.spark.sql.Row] = $anon.this.batchInputRowsIterator;
    private[this] val row: org.apache.spark.sql.Row = $anon.this.batchInputRowsIterator.head;
    &lt;stable&gt; &lt;accessor&gt; private def row: org.apache.spark.sql.Row = $anon.this.row;
    private[this] val schema: org.apache.spark.sql.types.StructType = $anon.this.row.schema;
    &lt;stable&gt; &lt;accessor&gt; private def schema: org.apache.spark.sql.types.StructType = $anon.this.schema;
    scala.this.Predef.assert($anon.this.schema.!=(null), &quot;Input rows must have a schema when prepending input rows to result rows&quot;);
    private[this] val newSchema: org.apache.spark.sql.types.StructType = org.apache.spark.sql.types.StructType.apply($anon.this.batchInputRowsIterator.head.schema.++[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](RequestBatchIterator.this.responseRowDeserializer.schema)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]));
    &lt;stable&gt; &lt;accessor&gt; private def newSchema: org.apache.spark.sql.types.StructType = $anon.this.newSchema;
    override def hasNext: Boolean = resultsIterator.hasNext;
    override def next(): org.apache.spark.sql.Row = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema($anon.this.batchInputRowsIterator.next().toSeq.++[Any, Seq[Any]](resultsIterator.next().toSeq)(collection.this.Seq.canBuildFrom[Any]).toArray[Any]((ClassTag.Any: scala.reflect.ClassTag[Any])), $anon.this.newSchema)
  };
  new $anon()
})
        </td>
      </tr><tr>
        <td>
          137
        </td>
        <td>
          2010
        </td>
        <td>
          6368
          -
          7084
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.currentResultsIterator_=
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.currentResultsIterator_=({
  final class $anon extends AnyRef with Iterator[org.apache.spark.sql.Row] {
    def &lt;init&gt;(): &lt;$anon: Iterator[org.apache.spark.sql.Row]&gt; = {
      $anon.super.&lt;init&gt;();
      ()
    };
    private[this] val batchInputRowsIterator: scala.collection.BufferedIterator[org.apache.spark.sql.Row] = batchInputRows.iterator.buffered;
    &lt;stable&gt; &lt;accessor&gt; private def batchInputRowsIterator: scala.collection.BufferedIterator[org.apache.spark.sql.Row] = $anon.this.batchInputRowsIterator;
    private[this] val row: org.apache.spark.sql.Row = $anon.this.batchInputRowsIterator.head;
    &lt;stable&gt; &lt;accessor&gt; private def row: org.apache.spark.sql.Row = $anon.this.row;
    private[this] val schema: org.apache.spark.sql.types.StructType = $anon.this.row.schema;
    &lt;stable&gt; &lt;accessor&gt; private def schema: org.apache.spark.sql.types.StructType = $anon.this.schema;
    scala.this.Predef.assert($anon.this.schema.!=(null), &quot;Input rows must have a schema when prepending input rows to result rows&quot;);
    private[this] val newSchema: org.apache.spark.sql.types.StructType = org.apache.spark.sql.types.StructType.apply($anon.this.batchInputRowsIterator.head.schema.++[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](RequestBatchIterator.this.responseRowDeserializer.schema)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]));
    &lt;stable&gt; &lt;accessor&gt; private def newSchema: org.apache.spark.sql.types.StructType = $anon.this.newSchema;
    override def hasNext: Boolean = resultsIterator.hasNext;
    override def next(): org.apache.spark.sql.Row = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema($anon.this.batchInputRowsIterator.next().toSeq.++[Any, Seq[Any]](resultsIterator.next().toSeq)(collection.this.Seq.canBuildFrom[Any]).toArray[Any]((ClassTag.Any: scala.reflect.ClassTag[Any])), $anon.this.newSchema)
  };
  new $anon()
})
        </td>
      </tr><tr>
        <td>
          138
        </td>
        <td>
          1993
        </td>
        <td>
          6452
          -
          6484
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.Iterator.buffered
        </td>
        <td style="background: #AEF1AE">
          batchInputRows.iterator.buffered
        </td>
      </tr><tr>
        <td>
          139
        </td>
        <td>
          1994
        </td>
        <td>
          6505
          -
          6532
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.BufferedIterator.head
        </td>
        <td style="background: #AEF1AE">
          $anon.this.batchInputRowsIterator.head
        </td>
      </tr><tr>
        <td>
          140
        </td>
        <td>
          1995
        </td>
        <td>
          6556
          -
          6566
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.Row.schema
        </td>
        <td style="background: #AEF1AE">
          $anon.this.row.schema
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          1996
        </td>
        <td>
          6584
          -
          6598
        </td>
        <td>
          Apply
        </td>
        <td>
          java.lang.Object.!=
        </td>
        <td style="background: #AEF1AE">
          $anon.this.schema.!=(null)
        </td>
      </tr><tr>
        <td>
          141
        </td>
        <td>
          1998
        </td>
        <td>
          6577
          -
          6686
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Predef.assert
        </td>
        <td style="background: #AEF1AE">
          scala.this.Predef.assert($anon.this.schema.!=(null), &quot;Input rows must have a schema when prepending input rows to result rows&quot;)
        </td>
      </tr><tr>
        <td>
          142
        </td>
        <td>
          1997
        </td>
        <td>
          6612
          -
          6685
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          &quot;Input rows must have a schema when prepending input rows to result rows&quot;
        </td>
      </tr><tr>
        <td>
          143
        </td>
        <td>
          2002
        </td>
        <td>
          6713
          -
          6805
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.types.StructType.apply
        </td>
        <td style="background: #AEF1AE">
          org.apache.spark.sql.types.StructType.apply($anon.this.batchInputRowsIterator.head.schema.++[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](RequestBatchIterator.this.responseRowDeserializer.schema)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]))
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          2000
        </td>
        <td>
          6771
          -
          6771
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField]
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          1999
        </td>
        <td>
          6774
          -
          6804
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.ResponseRowDeserializer.schema
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.responseRowDeserializer.schema
        </td>
      </tr><tr>
        <td>
          144
        </td>
        <td>
          2001
        </td>
        <td>
          6724
          -
          6804
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableLike.++
        </td>
        <td style="background: #AEF1AE">
          $anon.this.batchInputRowsIterator.head.schema.++[org.apache.spark.sql.types.StructField, Seq[org.apache.spark.sql.types.StructField]](RequestBatchIterator.this.responseRowDeserializer.schema)(collection.this.Seq.canBuildFrom[org.apache.spark.sql.types.StructField])
        </td>
      </tr><tr>
        <td>
          145
        </td>
        <td>
          2003
        </td>
        <td>
          6848
          -
          6871
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.Iterator.hasNext
        </td>
        <td style="background: #AEF1AE">
          resultsIterator.hasNext
        </td>
      </tr><tr>
        <td>
          147
        </td>
        <td>
          2008
        </td>
        <td>
          6923
          -
          7062
        </td>
        <td>
          Apply
        </td>
        <td>
          org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema.&lt;init&gt;
        </td>
        <td style="background: #AEF1AE">
          new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema($anon.this.batchInputRowsIterator.next().toSeq.++[Any, Seq[Any]](resultsIterator.next().toSeq)(collection.this.Seq.canBuildFrom[Any]).toArray[Any]((ClassTag.Any: scala.reflect.ClassTag[Any])), $anon.this.newSchema)
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          2006
        </td>
        <td>
          6964
          -
          7036
        </td>
        <td>
          ApplyToImplicitArgs
        </td>
        <td>
          scala.collection.TraversableOnce.toArray
        </td>
        <td style="background: #AEF1AE">
          $anon.this.batchInputRowsIterator.next().toSeq.++[Any, Seq[Any]](resultsIterator.next().toSeq)(collection.this.Seq.canBuildFrom[Any]).toArray[Any]((ClassTag.Any: scala.reflect.ClassTag[Any]))
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          2005
        </td>
        <td>
          6998
          -
          6998
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.collection.Seq.canBuildFrom
        </td>
        <td style="background: #AEF1AE">
          collection.this.Seq.canBuildFrom[Any]
        </td>
      </tr><tr>
        <td>
          148
        </td>
        <td>
          2004
        </td>
        <td>
          7001
          -
          7027
        </td>
        <td>
          Select
        </td>
        <td>
          org.apache.spark.sql.Row.toSeq
        </td>
        <td style="background: #AEF1AE">
          resultsIterator.next().toSeq
        </td>
      </tr><tr>
        <td>
          149
        </td>
        <td>
          2007
        </td>
        <td>
          7052
          -
          7061
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.$anon.newSchema
        </td>
        <td style="background: #AEF1AE">
          $anon.this.newSchema
        </td>
      </tr><tr>
        <td>
          153
        </td>
        <td>
          2012
        </td>
        <td>
          7108
          -
          7148
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.currentResultsIterator_=
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.currentResultsIterator_=(resultsIterator)
        </td>
      </tr><tr>
        <td>
          153
        </td>
        <td>
          2013
        </td>
        <td>
          7108
          -
          7148
        </td>
        <td>
          Block
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.currentResultsIterator_=
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.currentResultsIterator_=(resultsIterator)
        </td>
      </tr><tr>
        <td>
          156
        </td>
        <td>
          2018
        </td>
        <td>
          7167
          -
          7171
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          true
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2023
        </td>
        <td>
          7213
          -
          7213
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2019
        </td>
        <td>
          7217
          -
          7247
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.Iterator.hasNext
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.currentResultsIterator.hasNext
        </td>
      </tr><tr>
        <td>
          160
        </td>
        <td>
          2022
        </td>
        <td>
          7213
          -
          7213
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          161
        </td>
        <td>
          2021
        </td>
        <td>
          7257
          -
          7291
        </td>
        <td>
          Return
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.next
        </td>
        <td style="background: #AEF1AE">
          return RequestBatchIterator.this.currentResultsIterator.next()
        </td>
      </tr><tr>
        <td>
          161
        </td>
        <td>
          2020
        </td>
        <td>
          7264
          -
          7291
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.Iterator.next
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.currentResultsIterator.next()
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          2024
        </td>
        <td>
          7306
          -
          7314
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Boolean.unary_!
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.hasNext.unary_!
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          2027
        </td>
        <td>
          7302
          -
          7302
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          163
        </td>
        <td>
          2028
        </td>
        <td>
          7302
          -
          7302
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2026
        </td>
        <td>
          7324
          -
          7356
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          throw new scala.`package`.NoSuchElementException()
        </td>
      </tr><tr>
        <td>
          164
        </td>
        <td>
          2025
        </td>
        <td>
          7324
          -
          7356
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          throw new scala.`package`.NoSuchElementException()
        </td>
      </tr><tr>
        <td>
          166
        </td>
        <td>
          2029
        </td>
        <td>
          7367
          -
          7394
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.Iterator.next
        </td>
        <td style="background: #AEF1AE">
          RequestBatchIterator.this.currentResultsIterator.next()
        </td>
      </tr><tr>
        <td>
          179
        </td>
        <td>
          2030
        </td>
        <td>
          7904
          -
          7916
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Option.empty
        </td>
        <td style="background: #AEF1AE">
          scala.Option.empty[Nothing]
        </td>
      </tr><tr>
        <td>
          181
        </td>
        <td>
          2038
        </td>
        <td>
          7920
          -
          7920
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          181
        </td>
        <td>
          2031
        </td>
        <td>
          7924
          -
          7937
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.Iterator.hasNext
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.input.hasNext
        </td>
      </tr><tr>
        <td>
          181
        </td>
        <td>
          2037
        </td>
        <td>
          7920
          -
          7920
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #F0ADAD">
          ()
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          2033
        </td>
        <td>
          7964
          -
          7988
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.serializeRow
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.serializeRow(RequestRowIterator.this.input.next())
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          2036
        </td>
        <td>
          7945
          -
          7989
        </td>
        <td>
          Block
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.nextValue_=
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.nextValue_=(scala.Option.apply[(org.apache.spark.sql.Row, Array[Byte])](RequestRowIterator.this.serializeRow(RequestRowIterator.this.input.next())))
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          2035
        </td>
        <td>
          7945
          -
          7989
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.nextValue_=
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.nextValue_=(scala.Option.apply[(org.apache.spark.sql.Row, Array[Byte])](RequestRowIterator.this.serializeRow(RequestRowIterator.this.input.next())))
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          2032
        </td>
        <td>
          7977
          -
          7987
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.Iterator.next
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.input.next()
        </td>
      </tr><tr>
        <td>
          182
        </td>
        <td>
          2034
        </td>
        <td>
          7957
          -
          7989
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Option.apply[(org.apache.spark.sql.Row, Array[Byte])](RequestRowIterator.this.serializeRow(RequestRowIterator.this.input.next()))
        </td>
      </tr><tr>
        <td>
          189
        </td>
        <td>
          2039
        </td>
        <td>
          8134
          -
          8185
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.getOrElse
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.nextValue.map[Int](((x0$1: (org.apache.spark.sql.Row, Array[Byte])) =&gt; x0$1 match {
  case (_1: org.apache.spark.sql.Row, _2: Array[Byte])(org.apache.spark.sql.Row, Array[Byte])((a @ _), (b @ _)) =&gt; b.length
})).getOrElse[Int](0)
        </td>
      </tr><tr>
        <td>
          193
        </td>
        <td>
          2040
        </td>
        <td>
          8231
          -
          8249
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.nonEmpty
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.nextValue.nonEmpty
        </td>
      </tr><tr>
        <td>
          197
        </td>
        <td>
          2041
        </td>
        <td>
          8321
          -
          8334
        </td>
        <td>
          Select
        </td>
        <td>
          scala.Option.get
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.nextValue.get
        </td>
      </tr><tr>
        <td>
          198
        </td>
        <td>
          2042
        </td>
        <td>
          8342
          -
          8355
        </td>
        <td>
          Select
        </td>
        <td>
          scala.collection.Iterator.hasNext
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.input.hasNext
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2045
        </td>
        <td>
          8377
          -
          8409
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Option.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Option.apply[(org.apache.spark.sql.Row, Array[Byte])](RequestRowIterator.this.serializeRow(RequestRowIterator.this.input.next()))
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2044
        </td>
        <td>
          8384
          -
          8408
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.serializeRow
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.serializeRow(RequestRowIterator.this.input.next())
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2047
        </td>
        <td>
          8365
          -
          8409
        </td>
        <td>
          Block
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.nextValue_=
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.nextValue_=(scala.Option.apply[(org.apache.spark.sql.Row, Array[Byte])](RequestRowIterator.this.serializeRow(RequestRowIterator.this.input.next())))
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2043
        </td>
        <td>
          8397
          -
          8407
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.collection.Iterator.next
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.input.next()
        </td>
      </tr><tr>
        <td>
          199
        </td>
        <td>
          2046
        </td>
        <td>
          8365
          -
          8409
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.nextValue_=
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.nextValue_=(scala.Option.apply[(org.apache.spark.sql.Row, Array[Byte])](RequestRowIterator.this.serializeRow(RequestRowIterator.this.input.next())))
        </td>
      </tr><tr>
        <td>
          201
        </td>
        <td>
          2048
        </td>
        <td>
          8441
          -
          8453
        </td>
        <td>
          TypeApply
        </td>
        <td>
          scala.Option.empty
        </td>
        <td style="background: #AEF1AE">
          scala.Option.empty[Nothing]
        </td>
      </tr><tr>
        <td>
          201
        </td>
        <td>
          2050
        </td>
        <td>
          8429
          -
          8453
        </td>
        <td>
          Block
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.nextValue_=
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.nextValue_=(scala.Option.empty[Nothing])
        </td>
      </tr><tr>
        <td>
          201
        </td>
        <td>
          2049
        </td>
        <td>
          8429
          -
          8453
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.nextValue_=
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.nextValue_=(scala.Option.empty[Nothing])
        </td>
      </tr><tr>
        <td>
          207
        </td>
        <td>
          2051
        </td>
        <td>
          8562
          -
          8603
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.RequestRowSerializer.serializeRow
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.requestObjectSerializer.serializeRow(obj)
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2057
        </td>
        <td>
          8608
          -
          8608
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2053
        </td>
        <td>
          8612
          -
          8651
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Int.&gt;
        </td>
        <td style="background: #AEF1AE">
          serialized.length.&gt;(RequestRowIterator.this.maxBatchSizeInBytes)
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2056
        </td>
        <td>
          8608
          -
          8608
        </td>
        <td>
          Literal
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          ()
        </td>
      </tr><tr>
        <td>
          208
        </td>
        <td>
          2052
        </td>
        <td>
          8632
          -
          8651
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestRowIterator.maxBatchSizeInBytes
        </td>
        <td style="background: #AEF1AE">
          RequestRowIterator.this.maxBatchSizeInBytes
        </td>
      </tr><tr>
        <td>
          209
        </td>
        <td>
          2055
        </td>
        <td>
          8661
          -
          8869
        </td>
        <td>
          Block
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          throw new scala.`package`.IllegalArgumentException(&quot;Object serialized to byte array of length: &quot;.+(serialized.length).+(&quot; which is above max batch size in bytes of &quot;).+(RequestRowIterator.this.maxBatchSizeInBytes).+(&quot;. Object: &quot;).+(obj))
        </td>
      </tr><tr>
        <td>
          209
        </td>
        <td>
          2054
        </td>
        <td>
          8661
          -
          8869
        </td>
        <td>
          Throw
        </td>
        <td>
          &lt;nosymbol&gt;
        </td>
        <td style="background: #AEF1AE">
          throw new scala.`package`.IllegalArgumentException(&quot;Object serialized to byte array of length: &quot;.+(serialized.length).+(&quot; which is above max batch size in bytes of &quot;).+(RequestRowIterator.this.maxBatchSizeInBytes).+(&quot;. Object: &quot;).+(obj))
        </td>
      </tr><tr>
        <td>
          213
        </td>
        <td>
          2058
        </td>
        <td>
          8880
          -
          8897
        </td>
        <td>
          Apply
        </td>
        <td>
          scala.Tuple2.apply
        </td>
        <td style="background: #AEF1AE">
          scala.Tuple2.apply[org.apache.spark.sql.Row, Array[Byte]](obj, serialized)
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          2060
        </td>
        <td>
          9685
          -
          9685
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.&lt;init&gt;$default$7
        </td>
        <td style="background: #F0ADAD">
          util.this.RequestBatchIterator.&lt;init&gt;$default$7
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          2059
        </td>
        <td>
          9685
          -
          9685
        </td>
        <td>
          Select
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.&lt;init&gt;$default$6
        </td>
        <td style="background: #F0ADAD">
          util.this.RequestBatchIterator.&lt;init&gt;$default$6
        </td>
      </tr><tr>
        <td>
          233
        </td>
        <td>
          2061
        </td>
        <td>
          9685
          -
          9789
        </td>
        <td>
          Apply
        </td>
        <td>
          com.amazonaws.services.sagemaker.sparksdk.transformation.util.RequestBatchIterator.&lt;init&gt;
        </td>
        <td style="background: #F0ADAD">
          new RequestBatchIterator(endpointName, input, ser, de, prependResultRows, util.this.RequestBatchIterator.&lt;init&gt;$default$6, util.this.RequestBatchIterator.&lt;init&gt;$default$7)
        </td>
      </tr>
    </table>
          </div>
        </div>
      </body>
    </html>